{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52019763",
   "metadata": {},
   "source": [
    "## <a name=\"index\"></a> Indice\n",
    "\n",
    "https://platzi.com/p/mendezleonardom/curso/1511-big-data/diploma/detalle\n",
    "\n",
    "[¿Qué es Big Data?](#mark_00)\n",
    "\n",
    "[Introducción al manejo de datos en Cloud](#mark_01)\n",
    "\n",
    "[Datos en Cloud](#mark_02)\n",
    "\n",
    "[¿Qué nube debería utilizar en mi proyecto de Big Data?](#mark_03)\n",
    "\n",
    "[Arquitectura Lambda](#mark_04)\n",
    "\n",
    "[Arquitectura Kappa](#mark_05)\n",
    "\n",
    "[Arquitectura Batch](#mark_06)\n",
    "\n",
    "[Extracción --> Llevar tu información al cloud](#mark_07)\n",
    "\n",
    "[Demo - Creando nuestro IDE en la nube con Python - Boto3](#mark_08)\n",
    "\n",
    "[¿Cómo usar Boto3?](#mark_09)\n",
    "\n",
    "[Ejemplo Boto3 +AWS Athena Query ](#mark_9.1)\n",
    "\n",
    "[API Gateway](#mark_10)\n",
    "\n",
    "[Storage Gateway](#mark_11)\n",
    "\n",
    "[Kinesis Data Streams](#mark_12)\n",
    "\n",
    "[Configuración de Kinesis Data Streams](#mark_13)\n",
    "\n",
    "[Demo - Desplegando Kinesis con Cloudformation](#mark_14)\n",
    "\n",
    "<a name=\"index_01\"></a>\n",
    "\n",
    "[Kinesis Firehose](#mark_15)\n",
    "\n",
    "[Demo - Configuración de Kinesis Firehose](#mark_16)\n",
    "\n",
    "[AWS - MSK (Managed Streaming Kafka)](#mark_17)\n",
    "\n",
    "[Demo - Despliegue de un clúster con MSK](#mark_18)\n",
    "\n",
    "[AWS - Glue](#mark_19)\n",
    "\n",
    "[Demo - Instalando Apache Zeppelin](#mark_20)\n",
    "\n",
    "[Creación del Developer Endpoint](#mark_21)\n",
    "\n",
    "[Demo - Conectando nuestro developer Endpoint a nuestro Zeppelin Edpoint](#mark_22)\n",
    "\n",
    "[Demo - Creando nuestro primer ETL - Crawling](#mark_23)\n",
    "\n",
    "[Demo - Creando nuestro primer ETL - Ejecución](#mark_24)\n",
    "\n",
    "[Demo - Creando nuestro primer ETL - Carga](#mark_25)\n",
    "\n",
    "[AWS - EMR - Elastic Map Reduce](#mark_26)\n",
    "\n",
    "[Demo - Desplegando nuestro primer clúster con EMR](#mark_27)\n",
    "\n",
    "[Demo - Conectándonos a Apache Zeppelin en EMR](#mark_28)\n",
    "\n",
    "[Demo- Despliegue automático de EMR cluster con cloudformation](#mark_29)\n",
    "\n",
    "[AWS - Lambda](#mark_30)\n",
    "\n",
    "<a name=\"index_02\"></a>\n",
    "\n",
    "[Ejemplos AWS- Lambda](#mark_31)\n",
    "\n",
    "[Demo - Creando una lambda para BigData](#mark_32)\n",
    "\n",
    "[AWS - Athena (carga de info y consulta)](#mark_33)\n",
    "\n",
    "[Demo - Consultando data en S3 con Athena](#mark_34)\n",
    "\n",
    "[AWS - RedShift](#mark_35)\n",
    "\n",
    "[Demo - Creando nuestro primer clúster de RedShift](#mark_36)\n",
    "\n",
    "[AWS - Lake Formation](#mark_37)\n",
    "\n",
    "[AWS - ElasticSearch](#mark_38)\n",
    "\n",
    "[Demo - Creando nuestro primer clúster de ElasticSearch](#mark_39)\n",
    "\n",
    "[AWS - Kibana, visualización](#mark_40)\n",
    "\n",
    "[AWS - QuickSight, visualización para BI](#mark_41)\n",
    "\n",
    "[Demo - Visualizando nuestra data con QuickSight](#mark_42)\n",
    "\n",
    "[Seguridad en los Datos](#mark_43)\n",
    "\n",
    "[AWS Macie, seguridad en los Datos](#mark_44)\n",
    "\n",
    "[Demo - Configurando AWS Macie](#mark_45)\n",
    "\n",
    "[Apache Airflow, orquestación y automatización](#mark_46)\n",
    "\n",
    "<a name=\"index_03\"></a>\n",
    "\n",
    "[Demo - Creando nuestro primer clúster en Cloud Composer](#mark_47)\n",
    "\n",
    "[Arquitectura de referencia](#mark_48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f647d6",
   "metadata": {},
   "source": [
    "## <a name=\"mark_00\"></a> ¿Qué es Big Data?\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "BigData es un campo orientado al análisis, procesamiento y almacenamiento de grandes cantidades de información que usualmente provienen de múltiples fuentes y presentan diversas incompatibilidades.\n",
    "\n",
    "Cuando hablemos de este tema, debemos pensar en las cinco V de BigData.\n",
    "\n",
    "- Volumen: Por la gran cantidad de datos que debemos procesar.\n",
    "\n",
    "- Velocidad: ¿A qué nivel de velocidad necesito la información para obtener valor de ella?\n",
    "\n",
    "- Variedad: Podemos tener diversas fuentes de datos con diversos formatos de archivos y debemos ser capaces de tomar toda esta información y procesarla para generar valor.\n",
    "\n",
    "- Veracidad: Grantizar la consistencia que tenemos de estos datos que vamos a procesar.\n",
    "\n",
    "- Valor: ¿Cómo podemos generar valor a partir de esta información?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ef5da",
   "metadata": {},
   "source": [
    "## <a name=\"mark_01\"></a>Introducción al manejo de datos en Cloud\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "Cloud Computing en proyectos de Big Data Características generales:\n",
    "\n",
    "![](img_01.png)\n",
    "\n",
    "![](img_02.png)\n",
    "\n",
    "Ahorro buscamos --> costo por demanda.\n",
    "\n",
    "Flexibilidad --> podemos trabajar a nivel de multiples cloud providers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58094ac3",
   "metadata": {},
   "source": [
    "## <a name=\"mark_02\"></a>Datos en Cloud\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_03.png)\n",
    "\n",
    "Almacenamiento --> Elegir la mejor opción, S3, Cloud Provider, EBS, EFS, o un Volumen con HDFS (Sistema de archivos enfocado a procesamiento de información)\n",
    "\n",
    "Diferencias entre Extracción e Ingesta:\n",
    "\n",
    "    - Extracción nosotros extraemos/sacamos la data desde diferentes fuentes y la guardamos donde sea necesario para su posterior procesamiento (en teoría en cloud).\n",
    "    \n",
    "    - En la ingesta una fuente de datos X alimenta, o nos envia los datos al cloud, y nosotros en cloud colocamos distintos servicios para recibir esa información y comensar a procesarla.\n",
    "    \n",
    "![](img_04.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce43b9",
   "metadata": {},
   "source": [
    "## <a name=\"mark_03\"></a>¿Qué nube debería utilizar en mi proyecto de Big Data?\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "Actualmente el mercado de Cloud Computing tiene varios actores compitiendo entre sí por atraer la mayor cantidad de clientes a sus nubes, encontramos Múltiples opciones como: Amazon Web Services, Azure, Alibaba Cloud, Google Cloud Platform, Oracle Cloud, Rackspace, Digital Ocean y Softlayer entre muchas otras.\n",
    "\n",
    "Dentro de esta variedad de proveedores muchas veces es complejo tomar decisiones de cuál utilizar, el criterio para esta decisión puede estar dado por diferentes factores como:\n",
    "\n",
    "1. Costo: Valor de los servicios que serán utilizados en el proyecto.\n",
    "2. Tipo de pricing: Por demanda (por hora, minuto o segundo), subasta, reservado.\n",
    "3. Servicios: Variedad de servicios provistos por el cloud provider. ¿Cuál servicio se ajusta mejor a mis necesidades?\n",
    "4. Ubicación: Distribución de las regiones/zonas donde el cloud provider preste servicios por temas de latencia y experiencia usuario esto puede ser decisivo.\n",
    "5. Niveles de Servicio: Consultar la documentación por servicio y los niveles ofrecidos de disponibilidad.\n",
    "6. Soporte: Tipos de soporte, costo, tiempos de respuesta y nivel de soporte (basic, business, enterprise).\n",
    "7. Estudios de mercado: Revisar los diferentes estudios de mercado, por ejemplo: el cuadrante mágico de Gartner, en los cuales se evalúan en diferentes aspectos los servicios provistos.\n",
    "8. Documentación: Consultar la documentación de los cloud provider, muchas veces no es muy clara o está incompleta referente a sus servicios.\n",
    "\n",
    "![](img_05.png)\n",
    "![](img_06.png)\n",
    "![](img_07.png)\n",
    "\n",
    "\n",
    "Después de revisar las diferentes opciones que proveen los cloud providers encontramos variedad en servicios de acuerdo a su funcionalidad, otras nubes como Azure, Softlayer, Alibaba también cuentan con servicios orientados al procesamiento de datos, sin embargo dentro de su ecosistema no es tan completo el set de servicios, por tal motivo siempre que pensemos en proyectos de BigData los mejores cloud provider serán AWS y GCP.\n",
    "\n",
    "![](aws_azure_gcp_ocl_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35924813",
   "metadata": {},
   "source": [
    "## <a name=\"mark_04\"></a>Arquitectura Lambda\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "La arquitectura Lambda es atribuida a Nathan Marz, diseñada para ser escalable, tolerante a fallos y de alto procesamiento de datos.\n",
    "\n",
    "Tiene una gran robustez, puede procesar una alta cantidad de datos. \n",
    "\n",
    "Está compuesta por tres capas:\n",
    "\n",
    "1. Batch: En esta capa vamos a procesar toda la información almacenada con anterioridad, desde el día anterior hasta meses.\n",
    "\n",
    "2. Servicio/Serving: Dentro de esta capa es posible visualizar la data procesada de la capa batch.\n",
    "\n",
    "3. Speed: Conforme llega la data se va a ir procesando RealTime.\n",
    "\n",
    "![](img_08.png)\n",
    "\n",
    "### Observación:\n",
    "\n",
    "En el contexto de la tecnología, la palabra \"agnóstico\" se utiliza para describir una tecnología que no está ligada a una plataforma o proveedor específico. Esto significa que la tecnología puede utilizarse con cualquier plataforma o proveedor, sin necesidad de realizar cambios o adaptaciones.\n",
    "\n",
    "Por ejemplo, un lenguaje de programación agnóstico es un lenguaje que puede utilizarse con cualquier plataforma, como Java, Python o C++. Un servicio en la nube agnóstico es un servicio que puede utilizarse con cualquier proveedor de servicios en la nube, como Amazon Web Services, Microsoft Azure o Google Cloud Platform.\n",
    "\n",
    "La tecnología agnóstica tiene una serie de ventajas, entre las que se incluyen:\n",
    "\n",
    "* **Flexibilidad:** La tecnología agnóstica permite a los usuarios elegir la plataforma o proveedor que mejor se adapte a sus necesidades.\n",
    "* **Ahorro de costes:** La tecnología agnóstica puede ayudar a los usuarios a ahorrar costes al permitirles elegir el proveedor que ofrece las mejores tarifas.\n",
    "* **Mejor rendimiento:** La tecnología agnóstica puede ayudar a los usuarios a mejorar el rendimiento de sus aplicaciones al permitirles elegir la plataforma o proveedor que ofrece el mejor rendimiento para sus necesidades específicas.\n",
    "\n",
    "Algunos ejemplos de tecnologías agnósticas incluyen:\n",
    "\n",
    "* **Lenguajes de programación:** Java, Python, C++, Go\n",
    "* **Marcos de trabajo:** Spring Boot, Django, Laravel\n",
    "* **Bases de datos:** MySQL, PostgreSQL, MongoDB\n",
    "* **Almacenamiento en la nube:** Amazon S3, Microsoft Azure Blob Storage, Google Cloud Storage\n",
    "* **Computing en la nube:** Amazon EC2, Microsoft Azure Virtual Machines, Google Compute Engine\n",
    "\n",
    "La tecnología agnóstica es una tendencia cada vez más importante en el mundo de la tecnología. A medida que los usuarios exigen más flexibilidad y ahorro de costes, la tecnología agnóstica se convertirá en una opción más popular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd324885",
   "metadata": {},
   "source": [
    "## <a name=\"mark_05\"></a>Arquitectura Kappa\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_09.png)\n",
    "\n",
    "Fue presentada por Jay Krepsen en el 2014 como una evolución de la arquitectura lambda. Elimina la capa batch haciendo que todo se procese en tiempo real.\n",
    "\n",
    "La arquitectura Kappa sigue los siguientes pilares:\n",
    "\n",
    "1. Todo es un stream.\n",
    "\n",
    "2. Información de origen no modificada.\n",
    "\n",
    "3. Solo un (1) flujo de procesamiento.\n",
    "\n",
    "4. Capaz de reprocesar (podemos hacer lo que haciamos en la capa batch, utilizar información anterior para toma de desiciones).\n",
    "\n",
    "![](img_10.png)\n",
    "\n",
    "### Observación: \n",
    "\n",
    "En esta arquitectura cambian muchos de los servicios que utilizamos en el cloud provider, acá el enfoque está orientado a Streaming/RealTime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915f4d3",
   "metadata": {},
   "source": [
    "## <a name=\"mark_06\"></a>Arquitectura Batch\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_11.png)\n",
    "\n",
    "Input, Process, Output, estas 3 tareas debe ser orquestadas (servicio de orquestación de tareas), también tendremos que conectarlo con un sistema de almacenamiento (db, cloud storage, S3, volumenes con HDFS)\n",
    "\n",
    "Características:\n",
    "\n",
    "- La data que llega hoy será procesada mañana.\n",
    "\n",
    "- Procesamos data historica.\n",
    "\n",
    "- Si necesitamos agregar otra variable y reprocesar data historica, no hay inconveniente.\n",
    "\n",
    "- Podemos alimentar otros sistemas con esta data en batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f8da2",
   "metadata": {},
   "source": [
    "## <a name=\"mark_07\"></a>Extracción --> Llevar tu información al cloud\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "Existen distintas formas de conectar tu sistema en la nube con la/las fuente/s de origen de datos que necesitas, algunas opciones son:\n",
    "\n",
    "1. Utilizar el SDK de la nube utilizando el lenguaje de programación de tu preferencia.\n",
    "\n",
    "2. Usar el CLI para conectarse a la nube.\n",
    "\n",
    "3. Utilizar servicios especializados para la tarea.\n",
    "\n",
    "Es posible combinar esas opciones para integrarlos con algún servicio y llevar la información que necesites a tu cloud provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf7cc2",
   "metadata": {},
   "source": [
    "## <a name=\"mark_08\"></a>Demo - Creando nuestro IDE en la nube con Python - Boto3\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_13.png)\n",
    "\n",
    "![](img_14.png)\n",
    "\n",
    "Python es una gran opción para procesamiento de datos ya que cuenta con librerías como Pandas, Anaconda PyBrain, NumPy.\n",
    "\n",
    "Dentro de la consola de AWS --> Cloud9, este servicio es un IDE en la nube.\n",
    "\n",
    "![](img_15.png)\n",
    "\n",
    "Creamos un IDE completamente desde cero \"Create Environment\"\n",
    "\n",
    "![](img_16.png)\n",
    "\n",
    "En el siguiente paso tenemos la opción de crear una nueva instancia EC2 o utilizar una conexión SSH para manejarlo desde nuestro IDE local preferido.\n",
    "\n",
    "Para este ejemplo elegimos una instancia EC2 t2.micro, arrancamos desde la más pequeña, siempre recomendado.\n",
    "\n",
    "![](img_17.png)\n",
    "\n",
    "Siguiente tendremos Cost-saving setting, la instancia tiene varias opciónes para luego de inactividad apagarse y no generar costos.\n",
    "\n",
    "IAM Role, el servicio crea por nosotro el role necesario para utilizarlo.\n",
    "\n",
    "Networking setting (Advanced), elegimos la VPC para nuestra nueva instancia y la sub-red de ser necesario.\n",
    "\n",
    "![](img_18.png)\n",
    "\n",
    "![](img_19.png)\n",
    "\n",
    "Finalmente hacemos la review de los seteos y click en \"create environment\"\n",
    "\n",
    "Buscamos el servicio de Cloud9 y el ambiente creado.\n",
    "\n",
    "![](img_20.png)\n",
    "\n",
    "`aws s3 ls` --> listamos los buckets creados.\n",
    "\n",
    "Ahora abramos otra consola y nos vamos a \"Services\" --> EC2\n",
    "\n",
    "Tenemos nuestra instancia IDE creada\n",
    "\n",
    "![](img_24.png)\n",
    "\n",
    "Nuestro IDE debe interactuar con otros servicios con lo cual una recomendación es asignarle un role que se lo permita.\n",
    "\n",
    "![](img_25.png)\n",
    "\n",
    "### Observación:\n",
    "\n",
    "Verificar que el role que estamos usando tenga permisos sobre S3,configurar un role, asignar permisos sobre s3 y adjuntaselo a la EC2 del Cloud9.\n",
    "\n",
    "Volviendo a nuestro IDE crearemos un nuevo archivo.py y lo guardamos en nuestro proyecto.\n",
    "\n",
    "```python\n",
    "import boto3 #es la librería que se habla con todos los servicios de AWS.\n",
    "import sys\n",
    "import botocore\n",
    "\n",
    "\n",
    "region = sys.argv[1] #declaramos la región en la CLI, será su primer argumento \n",
    "\n",
    "s3 = boto3.client( #inicializamos el cliente de boto3 para que interactue con S3 \n",
    "  's3',\n",
    "  region_name = region\n",
    ")\n",
    "\n",
    "response = s3.list_buckets()\n",
    "print (response)\n",
    "```\n",
    "\n",
    "![](img_26.png)\n",
    "\n",
    "Agregamos la región manualmente (también podríamos correrlo desde la CLI)\n",
    "\n",
    "![](img_27.png)\n",
    "\n",
    "Algo muy importante es que podemos elegir el runner para correr nuestro código.\n",
    "\n",
    "![](img_28.png)\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "La librería **botocore** es una biblioteca de Python que proporciona una capa de abstracción para las API de Amazon Web Services (AWS). Esta biblioteca permite a los desarrolladores interactuar con los servicios de AWS de forma sencilla y eficiente, independientemente del lenguaje de programación que utilicen.\n",
    "\n",
    "La librería botocore proporciona una serie de características que facilitan el desarrollo de aplicaciones que utilizan AWS, entre las que se incluyen:\n",
    "\n",
    "* **Unificación de la API:** La librería botocore unifica las API de AWS en un conjunto de interfaces comunes. Esto permite a los desarrolladores aprender a utilizar una sola API para interactuar con todos los servicios de AWS.\n",
    "* **Soporte para múltiples idiomas:** La librería botocore está disponible para una serie de lenguajes de programación, incluyendo Python, Java, JavaScript, Ruby y Go. Esto permite a los desarrolladores elegir el lenguaje de programación que mejor se adapte a sus necesidades.\n",
    "* **Soporte para múltiples plataformas:** La librería botocore está disponible para múltiples plataformas, incluyendo Linux, macOS, Windows y Unix. Esto permite a los desarrolladores ejecutar sus aplicaciones en cualquier plataforma.\n",
    "\n",
    "La librería botocore es una herramienta esencial para cualquier desarrollador que quiera trabajar con AWS. Esta biblioteca proporciona una forma sencilla y eficiente de interactuar con los servicios de AWS, independientemente del lenguaje de programación que utilice.\n",
    "\n",
    "Aquí hay algunos ejemplos de cómo se puede utilizar la librería botocore:\n",
    "\n",
    "* **Para crear y administrar instancias de Amazon EC2:** La librería botocore proporciona una serie de métodos para crear, iniciar, detener y eliminar instancias de Amazon EC2.\n",
    "* **Para almacenar datos en Amazon S3:** La librería botocore proporciona una serie de métodos para cargar, descargar y gestionar datos en Amazon S3.\n",
    "* **Para enviar correos electrónicos con Amazon SES:** La librería botocore proporciona una serie de métodos para enviar correos electrónicos con Amazon SES.\n",
    "\n",
    "La librería botocore es una herramienta poderosa que puede utilizarse para una amplia gama de tareas en AWS.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://aws.amazon.com/es/cloud9/\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "https://github.com/boto/boto3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc455e2",
   "metadata": {},
   "source": [
    "## <a name=\"mark_09\"></a>¿Cómo usar Boto3?\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_29.png)\n",
    "\n",
    "Dentro del servicio de S3 podemos realizar todas las siguientes operaciones usando \"boto3\"\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/1.9.42/reference/services/s3.html\n",
    "\n",
    "![](img_30.png)\n",
    "\n",
    "![](img_31.png)\n",
    "\n",
    "`************************************************************************************************************************`\n",
    "\n",
    "Aquí hay un ejemplo de código en Python paso a paso de cómo especificar las ACL de un objeto S3 utilizando la biblioteca boto3:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "\n",
    "def set_acl(bucket_name, object_key, grants):\n",
    "    \"\"\"\n",
    "    Especifica las ACL de un objeto S3.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: El nombre del bucket S3.\n",
    "        object_key: La clave del objeto S3.\n",
    "        grants: Una lista de objetos Grant que representan las concesiones de acceso.\n",
    "\n",
    "    Returns:\n",
    "        El objeto de respuesta de la operación PutObjectAcl.\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client('s3')\n",
    "    response = client.put_object_acl(\n",
    "        Bucket=bucket_name,\n",
    "        Key=object_key,\n",
    "        AccessControlPolicy=grants\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    bucket_name = 'my-bucket'\n",
    "    object_key = 'my-object'\n",
    "\n",
    "    # Crea una lista de objetos Grant.\n",
    "    grants = [\n",
    "        {\n",
    "            'Grantee': {\n",
    "                'Type': 'AmazonCustomerByEmail',\n",
    "                'EmailAddress': 'user@example.com'\n",
    "            },\n",
    "            'Permission': 'READ'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Especifica las ACL del objeto.\n",
    "    response = set_acl(bucket_name, object_key, grants)\n",
    "\n",
    "    # Imprime el estado de la operación.\n",
    "    print(response['ResponseMetadata']['HTTPStatusCode'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Pasos:**\n",
    "\n",
    "1. **Importa la biblioteca boto3.**\n",
    "2. **Crea una función `set_acl()` que recibe como argumentos el nombre del bucket S3, la clave del objeto S3 y una lista de objetos Grant.**\n",
    "3. **En la función `set_acl()`, crea un cliente de S3.**\n",
    "4. **Utiliza el cliente de S3 para llamar al método `put_object_acl()`, proporcionando los argumentos especificados.**\n",
    "5. **Devuelve el objeto de respuesta de la operación.**\n",
    "6. **En el bloque `main()`, crea las variables `bucket_name`, `object_key` y `grants`.**\n",
    "7. **Utiliza las variables para llamar a la función `set_acl()`.**\n",
    "8. **Imprime el estado de la operación.**\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "La función `set_acl()` recibe como argumentos el nombre del bucket S3, la clave del objeto S3 y una lista de objetos Grant. Los objetos Grant representan las concesiones de acceso que se otorgarán al objeto S3.\n",
    "\n",
    "En el bloque `main()`, se crea una lista de objetos Grant con una sola concesión que otorga acceso de lectura al usuario `user@example.com`. Luego, se llama a la función `set_acl()` para especificar las ACL del objeto.\n",
    "\n",
    "El método `put_object_acl()` devuelve un objeto de respuesta que contiene información sobre la operación, como el código de estado HTTP. En el bloque `main()`, se imprime el estado de la operación para verificar que se realizó correctamente.\n",
    "\n",
    "**Puntos clave:**\n",
    "\n",
    "* **La lista de objetos Grant debe contener al menos una concesión.**\n",
    "* **El tipo de otorgante puede ser `AmazonCustomerByEmail`, `CanonicalUser` o `Group`.**\n",
    "* **El permiso puede ser `READ`, `WRITE`, `READ_ACP`, `WRITE_ACP` o `FULL_CONTROL`.**\n",
    "\n",
    "`************************************************************************************************************************`\n",
    "\n",
    "También podemos buscarlo para los servicios de EC2\n",
    "\n",
    "![](img_32.png)\n",
    "\n",
    "Dentro del servicio de EC\" podemos realizar todas las siguientes operaciones usando \"boto3\"\n",
    "\n",
    "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html\n",
    "\n",
    "![](img_33.png)\n",
    "\n",
    "`************************************************************************************************************************`\n",
    "\n",
    "Aquí hay un ejemplo de código en Python paso a paso de cómo crear una instancia de Amazon EC2 utilizando la biblioteca boto3:\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "\n",
    "def create_instance(instance_type, image_id, key_name, security_groups):\n",
    "    \"\"\"\n",
    "    Crea una instancia de Amazon EC2.\n",
    "\n",
    "    Args:\n",
    "        instance_type: El tipo de instancia EC2.\n",
    "        image_id: La ID de la imagen EC2.\n",
    "        key_name: El nombre de la clave SSH.\n",
    "        security_groups: Una lista de nombres de grupos de seguridad.\n",
    "\n",
    "    Returns:\n",
    "        El objeto de respuesta de la operación RunInstances.\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client('ec2')\n",
    "    response = client.run_instances(\n",
    "        InstanceType=instance_type,\n",
    "        ImageId=image_id,\n",
    "        KeyName=key_name,\n",
    "        SecurityGroupIds=security_groups\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    instance_type = 't2.micro'\n",
    "    image_id = 'ami-0123456789abcdef0'\n",
    "    key_name = 'my-key-pair'\n",
    "    security_groups = ['my-security-group']\n",
    "\n",
    "    # Crea la instancia.\n",
    "    response = create_instance(instance_type, image_id, key_name, security_groups)\n",
    "\n",
    "    # Imprime el ID de la instancia.\n",
    "    print(response['Instances'][0]['InstanceId'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Pasos:**\n",
    "\n",
    "1. **Importa la biblioteca boto3.**\n",
    "2. **Crea una función `create_instance()` que recibe como argumentos el tipo de instancia EC2, la ID de la imagen EC2, el nombre de la clave SSH y una lista de nombres de grupos de seguridad.**\n",
    "3. **En la función `create_instance()`, crea un cliente de EC2.**\n",
    "4. **Utiliza el cliente de EC2 para llamar al método `run_instances()`, proporcionando los argumentos especificados.**\n",
    "5. **Devuelve el objeto de respuesta de la operación.**\n",
    "6. **En el bloque `main()`, crea las variables `instance_type`, `image_id`, `key_name` y `security_groups`.**\n",
    "7. **Utiliza las variables para llamar a la función `create_instance()`.**\n",
    "8. **Imprime el ID de la instancia.**\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "La función `create_instance()` recibe como argumentos el tipo de instancia EC2, la ID de la imagen EC2, el nombre de la clave SSH y una lista de nombres de grupos de seguridad. Estos argumentos se utilizan para crear una instancia EC2.\n",
    "\n",
    "En el bloque `main()`, se crean las variables `instance_type`, `image_id`, `key_name` y `security_groups` con valores específicos. Luego, se llama a la función `create_instance()` para crear la instancia.\n",
    "\n",
    "El método `run_instances()` devuelve un objeto de respuesta que contiene información sobre la operación, como la ID de la instancia. En el bloque `main()`, se imprime el ID de la instancia para verificar que se creó correctamente.\n",
    "\n",
    "**Puntos clave:**\n",
    "\n",
    "* **El tipo de instancia EC2 define las características y el rendimiento de la instancia.**\n",
    "* **La ID de la imagen EC2 define el sistema operativo y las aplicaciones que se instalarán en la instancia.**\n",
    "* **El nombre de la clave SSH se utiliza para conectarse a la instancia.**\n",
    "* **Los grupos de seguridad definen los permisos de acceso a la instancia.**\n",
    "\n",
    "Este es solo un ejemplo de cómo se pueden utilizar las especificaciones de EC2 con boto3. Para obtener más información, consulte la documentación de boto3 para EC2: [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html).\n",
    "\n",
    "`************************************************************************************************************************`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db030aaa",
   "metadata": {},
   "source": [
    "## <a name=\"mark_9.1\"></a>Ejemplo Boto3 +AWS Athena Query \n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import os\n",
    "from retrying import retry\n",
    "\n",
    "\n",
    "def submit_query(query, database, s3_output):\n",
    "    \"\"\"Function for starting athena query\"\"\"\n",
    "    client = boto3.client('athena')\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "            },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_output,\n",
    "            'EncryptionConfiguration': {\n",
    "                'EncryptionOption': 'SSE_KMS',\n",
    "                'KmsKey': os.environ['KMSKEY']\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    print('Execution ID: ' + response['QueryExecutionId'])\n",
    "    return response\n",
    "```\n",
    "\n",
    "### Explicación paso a paso del código:\n",
    "\n",
    "**Importaciones:**\n",
    "\n",
    "* **boto3:** Permite interactuar con los servicios de Amazon Web Services (AWS) a través de Python.\n",
    "* **os:** Permite acceder a variables de entorno del sistema.\n",
    "* **retrying:** Provee la función `retry` para reintentar la ejecución del código en caso de errores transitorios.\n",
    "\n",
    "**Función `submit_query`:**\n",
    "\n",
    "**Propósito:**\n",
    "\n",
    "* Envía una consulta a Amazon Athena y devuelve la respuesta con el ID de ejecución de la consulta.\n",
    "\n",
    "**Argumentos:**\n",
    "\n",
    "* `query`: La consulta SQL a ejecutar.\n",
    "* `database`: El nombre de la base de datos en Athena donde ejecutar la consulta.\n",
    "* `s3_output`: La ubicación en Amazon S3 donde guardar los resultados de la consulta.\n",
    "\n",
    "**Pasos:**\n",
    "\n",
    "1. **Crea un cliente de Athena:**\n",
    "   - `client = boto3.client('athena')`: Instancia un cliente para interactuar con el servicio Athena de AWS.\n",
    "\n",
    "2. **Envía la consulta:**\n",
    "   - `response = client.start_query_execution(...)`: Ejecuta la consulta usando el cliente de Athena, proporcionando los siguientes parámetros:\n",
    "     - `QueryString`: La consulta SQL a ejecutar.\n",
    "     - `QueryExecutionContext`: El contexto de la consulta, incluyendo la base de datos.\n",
    "     - `ResultConfiguration`: La configuración de los resultados, incluyendo:\n",
    "       - `OutputLocation`: La ubicación en S3 para los resultados.\n",
    "       - `EncryptionConfiguration`: Configuración de encriptación con KMS.\n",
    "\n",
    "3. **Imprime el ID de ejecución:**\n",
    "   - `print('Execution ID: ' + response['QueryExecutionId'])`: Muestra en la consola el ID de la ejecución de la consulta.\n",
    "\n",
    "4. **Devuelve la respuesta:**\n",
    "   - `return response`: Devuelve el objeto de respuesta que contiene información sobre la ejecución de la consulta, como el ID de ejecución y el estado.\n",
    "\n",
    "**Puntos clave:**\n",
    "\n",
    "- **Encriptación de resultados:** Los resultados de la consulta se almacenan en S3 con encriptación SSE-KMS, usando la clave KMS especificada en la variable de entorno `KMSKEY`.\n",
    "- **Reintento en caso de errores:** La función `@retry` (no mostrada en el código proporcionado) permite reintentar la ejecución de la consulta en caso de errores transitorios, mejorando la resiliencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b8cf7",
   "metadata": {},
   "source": [
    "## <a name=\"mark_10\"></a>API Gateway\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "Uno de los servicio de extracción de información, lo veremos desde el punto de vista de Big Data, es el intermediario entre la data que estamos produciendo en N-fuentes y nuestros servicios cloud.\n",
    "\n",
    "![](img_34.png)\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Las llamadas recurrentes y concurrentes son dos tipos de llamadas a un servicio. La diferencia clave entre ellas es el momento en que se realizan las llamadas.\n",
    "\n",
    "**Llamadas recurrentes**\n",
    "\n",
    "Las llamadas recurrentes se realizan en intervalos regulares. Por ejemplo, una aplicación podría realizar una llamada recurrente a un servicio cada hora para obtener datos actualizados.\n",
    "\n",
    "Las llamadas recurrentes son útiles para tareas que deben realizarse con regularidad, como la actualización de datos o el envío de notificaciones.\n",
    "\n",
    "**Llamadas concurrentes**\n",
    "\n",
    "Las llamadas concurrentes se realizan al mismo tiempo. Por ejemplo, una aplicación podría realizar varias llamadas concurrentes a un servicio para procesar una gran cantidad de datos.\n",
    "\n",
    "Las llamadas concurrentes son útiles para tareas que requieren un alto rendimiento, como el procesamiento de transacciones o la carga de datos.\n",
    "\n",
    "**Resumen**\n",
    "\n",
    "| Característica | Llamadas recurrentes | Llamadas concurrentes |\n",
    "|---|---|---|\n",
    "| Momento en que se realizan | Intervalos regulares | Al mismo tiempo |\n",
    "| Uso | Tareas que deben realizarse con regularidad | Tareas que requieren un alto rendimiento |\n",
    "\n",
    "**Ejemplos**\n",
    "\n",
    "Aquí hay algunos ejemplos de llamadas recurrentes y concurrentes:\n",
    "\n",
    "**Llamadas recurrentes**\n",
    "\n",
    "* Una aplicación que actualiza el clima cada hora\n",
    "* Una aplicación que envía notificaciones a los usuarios cada día\n",
    "* Un servicio de monitoreo que recopila datos de rendimiento cada minuto\n",
    "\n",
    "**Llamadas concurrentes**\n",
    "\n",
    "* Un servicio de procesamiento de pagos que procesa transacciones\n",
    "* Un servicio de almacenamiento de datos que carga datos\n",
    "* Un servicio de análisis que procesa datos de big data\n",
    "\n",
    "La elección entre llamadas recurrentes y concurrentes depende de las necesidades específicas de la aplicación.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Los ataques DDoS, o ataques de denegación de servicio distribuidos, son ataques cibernéticos que intentan interrumpir el acceso a un sitio web, servicio o red. Esto se hace inundando el objetivo con una gran cantidad de tráfico de Internet.\n",
    "\n",
    "Hay dos tipos principales de ataques DDoS:\n",
    "\n",
    "* **Ataques de volumen:** Estos ataques utilizan un gran número de peticiones para sobrecargar el objetivo. Las peticiones pueden ser peticiones HTTP, peticiones DNS (Domain Name System) o cualquier otro tipo de petición.\n",
    "![](img_37.png)\n",
    "\n",
    "* **Ataques de aplicación:** Estos ataques explotan vulnerabilidades en una aplicación para enviar peticiones maliciosas al objetivo. Las peticiones maliciosas pueden sobrecargar la aplicación o causar daños.\n",
    "![](img_38.png)\n",
    "\n",
    "Los ataques DDoS pueden tener un impacto significativo en las organizaciones. Pueden causar pérdidas de ingresos, daños a la reputación y violaciones de datos.\n",
    "\n",
    "Para protegerse de los ataques DDoS, las organizaciones pueden tomar una serie de medidas, entre ellas:\n",
    "\n",
    "* **Implementar medidas de seguridad en la red:** Esto incluye el uso de firewalls, IPS y WAF para bloquear el tráfico malicioso.\n",
    "* **Diseñar aplicaciones seguras:** Esto incluye la implementación de controles de seguridad para evitar ataques de aplicación.\n",
    "* **Estar preparado para responder a ataques:** Esto incluye tener un plan de respuesta a incidentes DDoS.\n",
    "\n",
    "Las organizaciones también pueden recurrir a proveedores de servicios de seguridad para obtener protección contra los ataques DDoS.\n",
    "\n",
    "Los ataques DDoS son una amenaza creciente para las organizaciones de todos los tamaños. Es importante que las organizaciones estén preparadas para responder a estos ataques para proteger sus sistemas y datos.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Arquitectura en Big Data:\n",
    "\n",
    "Supongamos que tiene un sistema de gestion de tickets (Zendesk) y quieres procesar todos esos casos para extraer conclusiones que te permitan brindar un mejor servicio.\n",
    "\n",
    "Puedes configurar para que todos esos ticket, a través de una operación PUT, se envien al API Gateway\n",
    "\n",
    "El API Gateway va a ser capaz de tomarlo y enviarlos/treggerear una función Lambda.\n",
    "\n",
    "La función Lambda tiene la capacidad de interactuar/alimentar, utilizando Python (o cualquier SDK), con cualquiera de los servicios de información que tenemos en adelante (en AWS), Kinesis para procesar la data, un S3, un EMR, alimentando/transformando/visualizando.\n",
    "\n",
    "![](img_35.png)\n",
    "\n",
    "En el siguiente ejemplo podemos ver a la izquierda todos los productores de información, conectandose a API Gateway, y alimentar todos los servicios que siguen.\n",
    "\n",
    "![](img_36.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d866646e",
   "metadata": {},
   "source": [
    "## <a name=\"mark_11\"></a>Storage Gateway\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_39.png)\n",
    "\n",
    "Caso de uso: Si tenemos una App on-premise y queremos que todos los logs de esa App lleguen a AWS para iniciar el proceso de transformación y visualización de datos, podemos configurar nuestra App on-premise para que a travéz de Storage Gateway para todos esos logs/data llegue a AWS.\n",
    "\n",
    "Funcionamiento: Cuando estamos configurando AWS Storage Gateway, tenemos que descargar una imagen, usualmente es una VMDK (Virtual Machine Disk), se instala esta imagen en nuestro datacenter, y esto será la puerta de enlace entre on-premise y la nube. Desde la perspectiva de Big Data, puedes tener una App on-premise y a travéz de Storage Gateway mandar toda la información producida por los usuarios a AWS utilizando la VMDK.\n",
    "\n",
    "Entonces como quedaría la arquitectura:\n",
    "\n",
    "![](img_40.png)\n",
    "\n",
    "_ Data Center con nuestra App instalada.\n",
    "\n",
    "_ A travéz de NFS (Network File System) nos conectamos a AWS Storage Gateway (que será una VM desplegada en VMware), tomará todos los datos producidos y los envía a S3\n",
    "\n",
    "_ Luego activamos diferentes servicios para el procesamiento de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86271770",
   "metadata": {},
   "source": [
    "## <a name=\"mark_12\"></a>Kinesis Data Streams\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_41.png)\n",
    "\n",
    "_ La unidad fundamental dentro de Kinesis se llama Data Record.\n",
    "\n",
    "_ La información dentro de Kinesis por defecto solamente cuenta con un periodo de retención de 24 horas.\n",
    "\n",
    "_ El Shard es una secuencia de Data Records dentro de un stream.\n",
    "\n",
    "**Características de Kinesis Data Streams**:\n",
    "\n",
    "- Recopila y procesa grandes cantidades de stream de datos en tiempo real. Deben ser gigantescas cantidades de datos (TB, HB, con miles o millones de fuentes de información que alimentan el servicio).\n",
    "\n",
    "- Casos de uso: Procesamiento de logs, social media, market data feeds y web clickstream.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de la web, un **clickstream** es un registro de las acciones que un usuario realiza en una página web o aplicación. Este registro incluye información como las páginas que se visitan, los enlaces que se hacen clic, los formularios que se completan y los datos que se introducen.\n",
    "\n",
    "El clickstream se puede recopilar utilizando una variedad de técnicas, como las cookies, los registros del servidor web y los rastreadores de terceros. Esta información se puede utilizar para una variedad de propósitos, como el análisis del comportamiento del usuario, la optimización de la experiencia del usuario y la publicidad dirigida.\n",
    "\n",
    "Un **web clickstream** es una herramienta valiosa para los profesionales de marketing y los analistas de datos. Al analizar el clickstream, pueden comprender cómo los usuarios interactúan con su sitio web o aplicación. Esta información puede utilizarse para mejorar la experiencia del usuario, aumentar las conversiones y optimizar la estrategia de marketing.\n",
    "\n",
    "Aquí hay algunos ejemplos de cómo se puede utilizar el clickstream:\n",
    "\n",
    "* **Análisis del comportamiento del usuario:** El clickstream se puede utilizar para comprender cómo los usuarios navegan por un sitio web o aplicación. Esta información puede utilizarse para identificar las páginas que son más populares, los enlaces que los usuarios hacen clic con más frecuencia y los problemas que los usuarios pueden estar teniendo.\n",
    "* **Optimización de la experiencia del usuario:** El clickstream se puede utilizar para identificar áreas del sitio web o aplicación que pueden mejorarse. Por ejemplo, si los usuarios hacen clic con frecuencia en un enlace que no funciona, se puede corregir el enlace para mejorar la experiencia del usuario.\n",
    "* **Publicidad dirigida:** El clickstream se puede utilizar para crear anuncios dirigidos a usuarios específicos. Por ejemplo, si un usuario ha visitado una página sobre zapatos, se le puede mostrar un anuncio de zapatos en el futuro.\n",
    "\n",
    "El clickstream es una herramienta poderosa que puede utilizarse para una variedad de propósitos. Al analizar el clickstream, las empresas pueden comprender mejor a sus usuarios y tomar decisiones más informadas.\n",
    "\n",
    "Algunas herramientas específicas para el análisis de clickstream son:\n",
    "\n",
    "Google Analytics: Google Analytics es una herramienta gratuita de análisis web que ofrece una amplia gama de funciones para el análisis del comportamiento del usuario.\n",
    "Adobe Analytics: Adobe Analytics es una herramienta de análisis web comercial que ofrece funciones avanzadas para el análisis del comportamiento del usuario y la optimización de la experiencia del usuario.\n",
    "Matomo: Matomo es una herramienta de análisis web de código abierto que ofrece funciones similares a Google Analytics.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "- Kinesis Data Streams nos permite hacer agregaciones en el proceso de información; es decir, podemos juntar información, agruparla y hacer ciertas actividades con dicha información mientras es procesada.\n",
    "\n",
    "![](img_42.png)\n",
    "\n",
    "Kinesis como se ve en la imagen es muy flexible, interactuando con su API a travéz de la SDK podemos alimentarlo desde multiples fuentes.\n",
    "\n",
    "**Así está compuesto Kinesis Data Streams**:\n",
    "\n",
    "![](img_43.png)\n",
    "\n",
    "- Data Record: Es la unidad fundamental de almacenamiento de datos en Kinesis Data Streams.\n",
    "\n",
    "- Retention period (cuanto tiempo dura la data en Kinesis): El tiempo en que la información se mantiene accesible después de ser agregada al stream. Es muy importante tener en cuenta que Kinesis Data Streams no es un servicio para almacenar información, pero podemos contar con un periodo de retención, que por defecto es de 24 horas.\n",
    "\n",
    "- Producer: Es el encargado de poner el Data Record en Kinesis Data Streams.\n",
    "\n",
    "- Consumer: Toma los Data Records ya procesados de Kinesis Data Streams para alimentar otro sistema de información.\n",
    "\n",
    "- Shard: Es una secuencia de Data Records dentro de un stream. En la configuración de Kinesis, debemos especificar la cantidad de shards, y la cantidad de shard estará determinada principalmente por la cantidad del flujo de información que tengamos.\n",
    "\n",
    "- Partition key: Se usa para agrupar y organizar la data por shards, se encarga de tomar los Data Records y agruparlos en las diferentes shards que tengamos dentro de Kinesis.\n",
    "\n",
    "Otro ejemplo, producimos información en Stream, tomamos esa información, que será un Data Record que luego llegará a un Shard que agrupará los Data Records organizandolos por una Partition Key, esta información será procesada por Kinesis para poder alimentar otro tipo de sistemas, un Elastic Search, una Lambda, etc.\n",
    "\n",
    "![](img_44.png)\n",
    "\n",
    "Es así que Kinesis Data Streams se convierte en una parte fundamental de arquitecturas en tiempo real, arquitecturas en las cuales la necesidad principal es procesar millones de datos; se puede crecer N cantidad de shards y procesar de acuerdo a lo que permitan los shards en tamaño.\n",
    "\n",
    "Kinesis Data Stream nos brinda alta disponibilidad, redundancia y un performance muy alto para manejar grandes cantidades de datos en tiempo real.\n",
    "\n",
    "Lectura Recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d88da",
   "metadata": {},
   "source": [
    "## <a name=\"mark_13\"></a>Configuración de Kinesis Data Streams\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "![](img_45.png)\n",
    "\n",
    "![](img_46.png)\n",
    "\n",
    "Tendremos 4 tipos de Kinesis, para nuestro ejemplo utilizaremos Kinesis Streams.\n",
    "\n",
    "![](img_47.png)\n",
    "\n",
    "![](img_48.png)\n",
    "\n",
    "Vamos a darle un nombre a la Kinesis, y en el mismo paso podemos utilizar el estimador de Shards.\n",
    "\n",
    "![](img_49.png)\n",
    "\n",
    "Previamente tenemos que saber la cantidad de tráfico que nos llegará, la cantidad de Records.\n",
    "\n",
    "Ingresamos el tamaño de record promedio\n",
    "\n",
    "El máximo de records que se escribiran por segundo.\n",
    "\n",
    "En número de apps que consumirar los datos.\n",
    "\n",
    "Finalmente nos recomienda una cantidad de Shards, para el ejemplo recomendó 25, pero utilizaremos 5.\n",
    "\n",
    "![](img_50.png)\n",
    "\n",
    "Colocamos los 5 Shards y nos calcula automaticamente su capacidad.\n",
    "\n",
    "![](img_51.png)\n",
    "\n",
    "Se pueden procesar 5MB por segundo, equivalente a 5000 Records por segundo, con 10 MB de capacidad de lectura.\n",
    "\n",
    "Una vez creado, ingresamos a nuestro Kinesis Data Stream.\n",
    "\n",
    "![](img_52.png)\n",
    "\n",
    "En Details, podremos obtener el ARN (Amazon Resource Name), su Status, dos recomendaciones para enviar data al stream (la primera a travéz de la API y la segunda usando la librería KPL, librería que hay que instalar en nuestros productores para mandar la data a AWS), y también la posibilidad de conectar este Kinesis con los otros tipos de Kinesis existentes.\n",
    "\n",
    "![](img_53.png)\n",
    "\n",
    "Cantidad de Shards seteados.\n",
    "\n",
    "![](img_54.png)\n",
    "\n",
    "Podemos habilitar la encriptación de la data, ya que KMS tiene integración con Kinesis para agregar una capa de cifrado a la información.\n",
    "\n",
    "![](img_55.png)\n",
    "\n",
    "![](img_56.png)\n",
    "\n",
    "![](img_57.png)\n",
    "\n",
    "En la pestaña de monitoring, tendremos los diferentes gráficas que nos permitan monitorear el comportamiento.\n",
    "\n",
    "Pestaña tags, podemos etiquetar el recurso para al momento de sacar un reporte por tags.\n",
    "\n",
    "La pestaña Enhancen fan-out, hay app que requieren dedicados Shards que son hasta 2MB/seg, los cuales los podemos habilitar para que esos consumers reciban estás app, como una transferencia dedicada y garantizar que van a llegar 2 MB/seg de información."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12ef95",
   "metadata": {},
   "source": [
    "## <a name=\"mark_14\"></a>Demo - Desplegando Kinesis con Cloudformation\n",
    "\n",
    "## [Indice](#index)\n",
    "\n",
    "Ahora que ya sabemos como desplegar Kinesis en forma gráfica, lo haremos con Cloudformation.\n",
    "\n",
    "Cloudformation es un servicio que maneja infraestructura como código y nos va a permitir desplegar recursos en AWS partiendo de plantillas yaml o JSON \n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Cloudformation es un servicio de Amazon Web Services (AWS) que permite a los usuarios crear y administrar infraestructura en la nube como código. Esto significa que los usuarios pueden definir su infraestructura en un archivo de texto, en lugar de tener que realizar acciones manuales en la consola de AWS.\n",
    "\n",
    "Cloudformation utiliza un lenguaje de plantillas declarativo para definir la infraestructura. Las plantillas de Cloudformation son archivos de texto que describen los recursos que se deben crear, así como sus propiedades.\n",
    "\n",
    "Por ejemplo, una plantilla de Cloudformation podría definir una instancia de Amazon Elastic Compute Cloud (EC2) con la siguiente configuración:\n",
    "\n",
    "```\n",
    "Resources:\n",
    "  MyInstance:\n",
    "    Type: AWS::EC2::Instance\n",
    "    Properties:\n",
    "      InstanceType: t2.micro\n",
    "      ImageId: ami-0123456789abcdef0\n",
    "```\n",
    "\n",
    "Esta plantilla crearía una instancia EC2 con el tipo de instancia `t2.micro` y la imagen de AMI `ami-0123456789abcdef0`.\n",
    "\n",
    "Cloudformation utiliza estas plantillas para crear la infraestructura en la nube. Para crear una infraestructura, los usuarios simplemente cargan una plantilla de Cloudformation en la consola de AWS o utilizan la CLI de AWS.\n",
    "\n",
    "Cloudformation tiene una serie de ventajas sobre la administración manual de la infraestructura en la nube. Estas ventajas incluyen:\n",
    "\n",
    "* **Reducción de errores:** Cloudformation ayuda a reducir los errores al proporcionar una forma declarativa de definir la infraestructura.\n",
    "* **Automatización:** Cloudformation permite automatizar la creación y el mantenimiento de la infraestructura.\n",
    "* **Versionado:** Cloudformation permite versionar la infraestructura, lo que facilita el seguimiento de los cambios.\n",
    "* **Control de cambios:** Cloudformation permite controlar los cambios en la infraestructura, lo que ayuda a garantizar que los cambios se realicen de forma segura y controlada.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "## Plantillas \"Infraestructura como código\".\n",
    "\n",
    "### Plantilla master.yml:\n",
    "\n",
    "```yml\n",
    "AWSTemplateFormatVersion: 2010-09-09\n",
    "Description: Master stack file to create an infrastructure by stacks.\n",
    "Parameters:\n",
    "    \n",
    "    EnvironmentName: \n",
    "        Description: \"Select the Environment Name to Deploy\"\n",
    "        Type: String\n",
    "        Default: PREPROD\n",
    "        AllowedValues:\n",
    "            - DEV\n",
    "            - STG\n",
    "            - PRD\n",
    "            - PREPROD\n",
    "            - TEST\n",
    "    KinesisShardsNumber: \n",
    "        Description: \"Select the number of shards\"\n",
    "        Type: Number\n",
    "        Default: 1\n",
    "        \n",
    "Resources:\n",
    "    KinesisStream:\n",
    "        Type: \"AWS::CloudFormation::Stack\"\n",
    "        Properties:\n",
    "            TemplateURL: https://s3.amazonaws.com/my_bucket/rt/kinesis-distribution.yml\n",
    "            Parameters:\n",
    "                EnvironmentName: !Ref EnvironmentName\n",
    "                KinesisShardsNumber:\n",
    "                    !Ref KinesisShardsNumber\n",
    "```\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    " **Explicación paso a paso del código CloudFormation:**\n",
    "\n",
    "**1. Encabezado:**\n",
    "\n",
    "```yaml\n",
    "AWSTemplateFormatVersion: 2010-09-09\n",
    "Description: Master stack file to create an infrastructure by stacks.\n",
    "```\n",
    "\n",
    "- **AWSTemplateFormatVersion:** Indica la versión del formato de la plantilla CloudFormation que se utiliza (en este caso, la versión 2010-09-09).\n",
    "- **Description:** Ofrece una breve descripción del propósito de la plantilla (crear infraestructura mediante stacks).\n",
    "\n",
    "**2. Parámetros:**\n",
    "\n",
    "```yaml\n",
    "Parameters:\n",
    "\n",
    "    EnvironmentName:\n",
    "        Description: \"Select the Environment Name to Deploy\"\n",
    "        Type: String\n",
    "        Default: PREPROD\n",
    "        AllowedValues:\n",
    "            - DEV\n",
    "            - STG\n",
    "            - PRD\n",
    "            - PREPROD\n",
    "            - TEST\n",
    "    KinesisShardsNumber:\n",
    "        Description: \"Select the number of shards\"\n",
    "        Type: Number\n",
    "        Default: 1\n",
    "```\n",
    "\n",
    "- **Parameters:** Define valores que se pueden ingresar al momento de crear la infraestructura.\n",
    "    - **EnvironmentName:**\n",
    "        - Define el entorno a desplegar (DEV, STG, PRD, PREPROD o TEST).\n",
    "        - Tiene un valor predeterminado de PREPROD.\n",
    "        - También podríamos agregar condiciones extras basados en el ambiente, por ejemplo, que depende el ambiente seleccionado se despliegue en un ambiente diferente, en una cuenta diferente, en una VPC diferente.\n",
    "    - **KinesisShardsNumber:**\n",
    "        - Especifica el número de shards para el stream de Kinesis.\n",
    "        - Tiene un valor predeterminado de 1.\n",
    "\n",
    "**3. Recursos:**\n",
    "\n",
    "```yaml\n",
    "Resources:\n",
    "    KinesisStream:\n",
    "        Type: \"AWS::CloudFormation::Stack\"\n",
    "        Properties:\n",
    "            TemplateURL: https://s3.amazonaws.com/my_bucket/rt/kinesis-distribution.yml\n",
    "            Parameters:\n",
    "                EnvironmentName: !Ref EnvironmentName\n",
    "                KinesisShardsNumber: !Ref KinesisShardsNumber\n",
    "```\n",
    "\n",
    "- **Resources:** Es la sección principal donde se definen los recursos que se crearán en AWS.\n",
    "    - **KinesisStream:**\n",
    "        - Define un stack de CloudFormation anidado, lo que significa que creará recursos a partir de otra plantilla CloudFormation.\n",
    "        - **Type:** \"AWS::CloudFormation::Stack\" indica que se trata de un stack anidado.\n",
    "        - **Properties:**\n",
    "            - **TemplateURL:** La URL de la plantilla CloudFormation que se utilizará para crear los recursos del stack anidado (en este caso, ubicada en un bucket S3).\n",
    "            - **Parameters:** Parámetros que se pasarán a la plantilla \"kinesis-distribution.yml\" anidada, utilizando referencias a los parámetros definidos en la plantilla principal, `!Ref EnvironmentName` pasar su valor por default \"PREPROD\", y `!Ref KinesisShardsNumber` su valor por default \"1\".**\n",
    "\n",
    "**En resumen:**\n",
    "\n",
    "Esta plantilla CloudFormation principal sirve como un orquestador. Define parámetros para el entorno y el número de shards de Kinesis, y luego crea un stack anidado que se encargará de crear los recursos de Kinesis específicos utilizando la plantilla CloudFormation ubicada en S3.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Plantilla kinesis-distribution.yml que recibe los parámetros desde master.yml para desplegar el servicio:\n",
    "\n",
    "```yml\n",
    "Description: >\n",
    "    This template deploys two different Amazon Kinesis Streams we need for the architecture.\n",
    "Parameters:\n",
    "    EnvironmentName:\n",
    "        Description: An environment name that will be prefixed to resource names\n",
    "        Type: String\n",
    "    KinesisShardsNumber: \n",
    "        Description: \"Select the number of shards\"\n",
    "        Type: Number\n",
    "Resources:\n",
    "  KinesisRT:\n",
    "    Type: 'AWS::Kinesis::Stream'\n",
    "    Properties:\n",
    "      Name: !Sub ${EnvironmentName}-KinesisRT\n",
    "      RetentionPeriodHours: 24\n",
    "      ShardCount: \n",
    "        !Ref KinesisShardsNumber\n",
    "      Tags:\n",
    "        - Key: Name\n",
    "          Value: !Sub ${EnvironmentName}-RealTime\n",
    "\n",
    "\n",
    "Outputs:\n",
    "\n",
    "  KinesisRT:\n",
    "    Description: A reference to the Kinesis Stream containing all events from reactive application.\n",
    "    Value: !Ref KinesisRT\n",
    "```\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "**Explicación paso a paso del código CloudFormation:**\n",
    "\n",
    "**1. Descripción:**\n",
    "\n",
    "```yaml\n",
    "Description: >\n",
    "    This template deploys two different Amazon Kinesis Streams we need for the architecture.\n",
    "```\n",
    "\n",
    "- **Description:** Describe el propósito de la plantilla CloudFormation (desplegar dos streams de Kinesis para la arquitectura).\n",
    "\n",
    "**2. Parámetros:**\n",
    "\n",
    "```yaml\n",
    "Parameters:\n",
    "    EnvironmentName:\n",
    "        Description: An environment name that will be prefixed to resource names\n",
    "        Type: String\n",
    "    KinesisShardsNumber:\n",
    "        Description: \"Select the number of shards\"\n",
    "        Type: Number\n",
    "```\n",
    "\n",
    "- **Parameters:** Define valores que se pueden ingresar al momento de crear la infraestructura.\n",
    "    - **EnvironmentName:**\n",
    "        - Será un prefijo para los nombres de los recursos.\n",
    "        - Tipo de dato: String (cadena de texto).\n",
    "    - **KinesisShardsNumber:**\n",
    "        - Especifica el número de shards para los streams de Kinesis.\n",
    "        - Tipo de dato: Number (número).\n",
    "\n",
    "**3. Recursos:**\n",
    "\n",
    "```yaml\n",
    "Resources:\n",
    "  KinesisRT:\n",
    "    Type: 'AWS::Kinesis::Stream'\n",
    "    Properties:\n",
    "      Name: !Sub ${EnvironmentName}-KinesisRT\n",
    "      RetentionPeriodHours: 24\n",
    "      ShardCount: !Ref KinesisShardsNumber\n",
    "      Tags:\n",
    "        - Key: Name\n",
    "          Value: !Sub ${EnvironmentName}-RealTime\n",
    "```\n",
    "\n",
    "- **Resources:** Define los recursos que se crearán en AWS.\n",
    "    - **KinesisRT:**\n",
    "        - Es el nombre elegido para crear el recurso Kinesis.\n",
    "        - Crea un stream de Kinesis.\n",
    "        - **Type:** 'AWS::Kinesis::Stream' indica que se trata de un stream de Kinesis.\n",
    "        - **Properties:**\n",
    "            - **Name:** El nombre del stream, construido con el prefijo que enviamos en la plantilla Master `!Sub ${EnvironmentName}-KinesisRT`, substituye el prefijo por el valor enviado en `EnvironmentName` desde \"master.yml\".\n",
    "            - **RetentionPeriodHours:** El periodo de retención de los datos en horas (24 en este caso).\n",
    "            - **ShardCount:** El número de shards, tomando el valor del parámetro `KinesisShardsNumber` de la plantilla Master.\n",
    "            - **Tags:** Etiquetas para identificar el recurso utiliza el \"EnvironmentName\" de la plantilla Master como prefijo, `!Sub ${EnvironmentName}-KinesisRT`, substituye el prefijo por el valor enviado en `EnvironmentName` desde \"master.yml\".\n",
    "\n",
    "**4. Outputs:**\n",
    "\n",
    "```yaml\n",
    "Outputs:\n",
    "  KinesisRT:\n",
    "    Description: A reference to the Kinesis Stream containing all events from reactive application.\n",
    "    Value: !Ref KinesisRT\n",
    "```\n",
    "\n",
    "- **Outputs:** Luego de creado el recurso se toman los valores exportandolo en una parte de Cloudformation, si en el futuro queremos que otro servicio se conecte a este Kinesis, podemos ir a los outputs recuperar la información del Kinesis y la ajustamos en el nuevo template, entonces permite exportar valores para ser utilizados en otras plantillas o scripts.\n",
    "    - **KinesisRT:** Exporta una referencia al stream de Kinesis creado, permitiendo su uso en otras partes de la infraestructura.\n",
    "\n",
    "**En resumen:**\n",
    "\n",
    "Esta plantilla CloudFormation crea un stream de Kinesis llamado `${EnvironmentName}-KinesisRT` con un periodo de retención de 24 horas y el número de shards especificado como parámetro. El stream se etiqueta con el nombre `${EnvironmentName}-RealTime`. La plantilla exporta una referencia al stream para su uso en otras partes de la infraestructura.\n",
    "\n",
    "**Puntos adicionales:**\n",
    "\n",
    "- La plantilla no crea el segundo stream de Kinesis mencionado en la descripción.\n",
    "- El código utiliza la función `!Sub` para realizar sustituciones/substituciones de variables dentro de los valores de las propiedades.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Una vez que tenemos los templates \"master.yml\" y \"kinesis-distribution.yml\" creados, los cargamos en S3\n",
    "\n",
    "![](img_58.png)\n",
    "\n",
    "Abrimos la plantilla Master para copiar su ruta, está ruta es la que utilizaremos en Cloudformation\n",
    "\n",
    "![](img_59.png)\n",
    "\n",
    "Nuevamente en la consola buscamos \"CloudFormation\", lo abrimos y creamos un nuevo stack\n",
    "\n",
    "![](img_60.png)\n",
    "\n",
    "![](img_61.png)\n",
    "\n",
    "![](img_62.png)\n",
    "\n",
    "En step de \"options\" no se realizaron seteos --> \"Next\".\n",
    "\n",
    "![](img_63.png)\n",
    "\n",
    "Y finalmente damos click en \"Create\" sin tildar nada\n",
    "\n",
    "![](img_64.png)\n",
    "\n",
    "Una vez creador en CloudFormation, vamos a la consola y buscamos \"Kinesis\", ya veremos nuestro recurso Kinesis creado.\n",
    "\n",
    "![](img_65.png)\n",
    "\n",
    "### Observación_01:\n",
    "\n",
    "Una forma de uso ...\n",
    "\n",
    "Tenemos todos nuestros templates en un repositorio, en el cual el versionado sea manejado por Git, luego conectar nuestro repo a un Code-pipeline dentro de Amazon, este Code-pipeline toma la plantilla necesaria y crea un artefacto a travez de AWS CodeBuild y CodeBuild se lo pasa a CloudFormation para que finalmente despliegue los recursos.\n",
    "\n",
    "Es así que con estos templates podemos desplegar código en la región que sea necesaria.\n",
    "\n",
    "### Observación_02:\n",
    "\n",
    "Si un template es muuuuyyy grande entonces debemos pensar en la idea de utilizar varios templates que se conecten unos a otros como building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bebbb2",
   "metadata": {},
   "source": [
    "## <a name=\"mark_15\"></a>Kinesis Firehose\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "![](img_66.png)\n",
    "\n",
    "Se puede convinar Kinesis Data Stream --> Kinesis Firehose\n",
    "\n",
    "![](img_67.png)\n",
    "\n",
    "Para este ejemplo, dentro del cluster de Elasticsearch vamos a tener \"índices\", se debe tener en cuenta que por cada índice vamos a necesitar un Kinesis Firehose diferente.\n",
    "\n",
    "Otro ejemplo:\n",
    "\n",
    "![](img_68.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0945aa",
   "metadata": {},
   "source": [
    "## <a name=\"mark_16\"></a>Demo - Configuración de Kinesis Firehose\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Vamos a la consola y buscamos \"Kinesis\", crearemos un \"Delivery stream\"\n",
    "\n",
    "![](img_69.png)\n",
    "\n",
    "![](img_70.png)\n",
    "\n",
    "![](img_71.png)\n",
    "\n",
    "Si elegimos PUT, tendremos las siguientes alternativas para alimentar el servicio.\n",
    "\n",
    "![](img_72.png)\n",
    "\n",
    "Click en Next.\n",
    "\n",
    "![](img_73.png)\n",
    "\n",
    "![](img_74.png)\n",
    "\n",
    "![](img_75.png)\n",
    "\n",
    "Parquet ORC son archivos comprimidos, más sencillos que JSON.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    " **Optimized Row Columnar (ORC) conceptos:**\n",
    "\n",
    "**¿Qué es ORC?**\n",
    "\n",
    "- Es un formato de almacenamiento de datos de código abierto y optimizado para columnas.\n",
    "- Se diseñó para superar las limitaciones de otros formatos de Hive y mejorar el rendimiento en la lectura, escritura y procesamiento de datos.\n",
    "- Es compatible con Apache Hive, Apache Spark, Apache Flink, Apache Hadoop y otros frameworks de big data.\n",
    "\n",
    "**¿Cómo funciona ORC?**\n",
    "\n",
    "- **Organización en columnas:** En lugar de almacenar los datos fila por fila, ORC los almacena columna por columna. Esto permite:\n",
    "    - Lecturas más rápidas de columnas específicas.\n",
    "    - Compresión más eficiente, ya que los valores similares se agrupan.\n",
    "- **Codificacion eficiente:** ORC utiliza varias técnicas de compresión y codificación para reducir el tamaño de los archivos y acelerar las operaciones:\n",
    "    - Codificación de longitud de ejecución (RLE) para valores repetidos.\n",
    "    - Codificación de diccionario para valores comunes.\n",
    "    - Codificación delta para valores numéricos.\n",
    "- **Metadatos enriquecidos:** ORC incluye metadatos detallados sobre el esquema de los datos, las estadísticas de las columnas y los índices. Estos metadatos permiten:\n",
    "    - Optimizar las consultas.\n",
    "    - Saltar a las filas o columnas relevantes sin leer todo el archivo.\n",
    "\n",
    "**Ventajas de ORC:**\n",
    "\n",
    "- **Rendimiento:** Lecturas y consultas más rápidas, especialmente en análisis que involucran pocas columnas.\n",
    "- **Compresión:** Tamaños de archivo reducidos, ahorrando espacio de almacenamiento y mejorando la transferencia de datos.\n",
    "- **Escalabilidad:** Maneja grandes conjuntos de datos de forma eficiente.\n",
    "- **Interoperabilidad:** Compatible con múltiples frameworks de big data.\n",
    "- **ACID:** Soporte para transacciones ACID y aislamiento de instantáneas.\n",
    "- **Tipos de datos complejos:** Admite structs, listas, mapas y uniones.\n",
    "\n",
    "**Usos frecuentes:**\n",
    "\n",
    "- Almacenamiento de datos en Apache Hive.\n",
    "- Procesamiento de datos con Apache Spark, Apache Flink y otros frameworks.\n",
    "- Análisis de datos en sistemas de big data.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Ahora debemos seleccionar el destino, a donde vamos a mandar nuestra data transformada.\n",
    "\n",
    "![](img_76.png)\n",
    "\n",
    "## Si seleccionamos S3, debemos especificar el bucket y si es necesario su prefijo.\n",
    "\n",
    "![](img_77.png)\n",
    "\n",
    "Las limitaciones.\n",
    "\n",
    "![](img_87.png)\n",
    "\n",
    "![](img_88.png)\n",
    "\n",
    "![](img_89.png)\n",
    "\n",
    "Para todas las acciones anteriores se requiere un Role, que le de los permisos a Kinesis para interactuar con los otros servicios.\n",
    "\n",
    "Si elegimos la opción de crear nuevo role, nos va a saltar una nueva ventana que nos pedirá permiso para crear el nuevo IAM role, simplemente le damos al botón Permitir.\n",
    "\n",
    "![](img_90.png)\n",
    "\n",
    "Nos debe quedar un IAM role de la siguiente manera, ahora damos click a Next.\n",
    "\n",
    "![](img_91.png)\n",
    "\n",
    "Nos aparecerá una review de la configuración del delivery stream, solamente damos click en el botón Create delivery stream.\n",
    "\n",
    "Esperamos a que termine de crearse nuestro delivery stream y vamos a proceder a probar que nuestro delivery stream funcione correctamente. Damos click en el nombre de nuestro delivery.\n",
    "\n",
    "![](img_92.png)\n",
    "\n",
    "Nos encontraremos en una página con toda la información acerca de nuestro stream. Abajo del nombre veremos un texto que dice Test with demo data, le vamos a dar click y nos va a desplegar información sobre la prueba que va a realizar.\n",
    "\n",
    "La prueba consta de mandar un simple archivo a nuestro S3. Debemos dar click en el botón Start sending data para iniciar la transmisión de información.\n",
    "\n",
    "![](img_93.png)\n",
    "\n",
    "Este proceso puede llegar a tardar unos minutos, aproximadamente 3-5 minutos después vamos a dar click al enlace de nuestro bucket S3.\n",
    "\n",
    "![](img_94.png)\n",
    "\n",
    "Si aún no encuentras nada de información dentro de tu bucket recuerda ser paciente, la velocidad de transmisión depende de la configuración que hicimos al buffer, en este caso dejamos la configuración por defecto de 5MB.\n",
    "\n",
    "Tras unos minutos debe aparecerte una carpeta dentro de otra y otra, separando la información transmitida por su fecha. Dentro encontrarás el archivo que mandaste, solamente queda regresar a la prueba de nuestro stream y apretar el botón Stop sending demo data.\n",
    "\n",
    "![](img_95.png)\n",
    "\n",
    "## Si seleccionamos \"Amazon Redshift\"\n",
    "\n",
    "![](img_78.png)\n",
    "\n",
    "Especificamos el cluster la base de datos, la tabla la columna, y \"el bucket S3 intermedio de la base de datos\".\n",
    "\n",
    "![](img_79.png)\n",
    "\n",
    "![](img_80.png)\n",
    "\n",
    "## Si seleccionamos Amazon Elasticsearch Service.\n",
    "\n",
    "![](img_81.png)\n",
    "\n",
    "![](img_82.png)\n",
    "\n",
    "A travéz de indices va a recopilar información y a travéz de Kibana nos permite hacer consultas a esa información. Rotación del índice (no vamos a guardar esa información por siempre), si tenemos un problema en el endpoint de destino, podemos configurar para que la data se vaya al bucket de S3 y no se pierda y luego la reprocesamos.\n",
    "\n",
    "![](img_83.png)\n",
    "\n",
    "## Si elegimos \"Splunk\"\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de AWS, Splunk es una plataforma de inteligencia de datos y seguridad que permite recopilar, analizar y visualizar datos de una amplia gama de fuentes, incluidas aplicaciones, infraestructura, redes y dispositivos IoT. Splunk puede utilizarse para una variedad de propósitos, como:\n",
    "\n",
    "* **Monitoreo de rendimiento:** Splunk puede utilizarse para recopilar y analizar datos de rendimiento de aplicaciones, infraestructura y redes. Esto puede ayudar a identificar problemas de rendimiento y tomar medidas para remediarlos.\n",
    "* **Seguridad:** Splunk puede utilizarse para recopilar y analizar datos de seguridad, como registros de auditoría, registros de eventos y datos de tráfico de red. Esto puede ayudar a detectar amenazas de seguridad y tomar medidas para mitigarlas.\n",
    "* **Analítica empresarial:** Splunk puede utilizarse para recopilar y analizar datos de negocio, como datos de ventas, datos de clientes y datos de marketing. Esto puede ayudar a las empresas a tomar mejores decisiones comerciales.\n",
    "\n",
    "Splunk ofrece una variedad de soluciones en la nube que pueden implementarse en AWS. Estas soluciones incluyen:\n",
    "\n",
    "* **Splunk Cloud:** Una solución de inteligencia de datos y seguridad completamente administrada que se ejecuta en AWS.\n",
    "* **Splunk Enterprise Cloud:** Una solución de inteligencia de datos y seguridad que se ejecuta en AWS y proporciona un mayor control y flexibilidad que Splunk Cloud.\n",
    "* **Splunk Infrastructure Monitoring:** Una solución de monitoreo de rendimiento para infraestructura que se ejecuta en AWS.\n",
    "* **Splunk Security Analytics:** Una solución de seguridad para la recopilación, el análisis y la visualización de datos de seguridad que se ejecuta en AWS.\n",
    "\n",
    "Splunk es una herramienta poderosa que puede utilizarse para una variedad de propósitos en AWS. Su amplia gama de capacidades y su flexibilidad la convierten en una opción adecuada para organizaciones de todos los tamaños.\n",
    "\n",
    "A continuación, se presentan algunos ejemplos específicos de cómo se puede utilizar Splunk en AWS:\n",
    "\n",
    "* **Una empresa de comercio electrónico puede utilizar Splunk para recopilar y analizar datos de tráfico web, datos de ventas y datos de clientes. Esto puede ayudar a la empresa a identificar tendencias de compra, optimizar sus campañas de marketing y mejorar la experiencia del cliente.**\n",
    "* **Una organización gubernamental puede utilizar Splunk para recopilar y analizar datos de seguridad, como registros de auditoría, registros de eventos y datos de tráfico de red. Esto puede ayudar a la organización a detectar amenazas de seguridad y proteger sus sistemas e información.**\n",
    "* **Una empresa de fabricación puede utilizar Splunk para recopilar y analizar datos de producción, como datos de máquinas, datos de procesos y datos de calidad. Esto puede ayudar a la empresa a mejorar la eficiencia de su producción y garantizar la calidad de sus productos.**\n",
    "\n",
    "Las posibilidades de utilizar Splunk en AWS son infinitas. La plataforma es lo suficientemente flexible para adaptarse a las necesidades de cualquier organización.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "![](img_84.png)\n",
    "\n",
    "![](img_85.png)\n",
    "\n",
    "![](img_86.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf27e8f",
   "metadata": {},
   "source": [
    "## <a name=\"mark_17\"></a>AWS - MSK (Managed Streaming Kafka)\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "![](img_96.png)\n",
    "\n",
    "![](img_97.png)\n",
    "\n",
    "Es la idea que el cluster esté replicado en varias zonas para su alta disponibilidad, al crear el clúster debemos especificar la cantidad de nodos por zona de disponibilidad. Con esto, el clúster se convierte en un multi-zona brindándonos una muy alta disponibilidad.\n",
    "\n",
    "Zookeper es un servicio que nos ayuda a mantener la configuración, datos de nombres, sincronización, rotación e integración con otros servicios.\n",
    "\n",
    "Por defecto, al crear el clúster de MSK, se crea un nodo de Zookeeper.\n",
    "\n",
    "Apache Kafka administrado en la nube, cuando comenzamos a comparar los servicios de MSK y Kinesis puede que haya casos de uso en el cual sea muy complejo cual de los dos se va a usar. Entonces como seleccionar que servicio usar (según AWS), si vas a usar Kinesis es porque necesitas una fuerte integración con otros servicios dentro de AWS, por el contrario si vas a usar MSK es porque vas a tener una fuerte integración con sistemas de terceros, con open source, lo cual nos brinda una mayor flexibilidad que también viene asociada a una mayor carga administrativa debido a que lo que desplegamos en MSK es un cluster.\n",
    "\n",
    "Con lo cual si tu empresa tiene Apache Kafka on-premise, este puede ser subido a la nube.\n",
    "\n",
    "Se desplegará en un cluster compuesto por \"Broke nodes\" y \"Zookeeper nodes\" para orquestar el procesamiento de data:\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Apache Kafka es una plataforma de transmisión de datos de código abierto que se utiliza para recopilar, almacenar y procesar grandes cantidades de datos en tiempo real.\n",
    "\n",
    "Kafka está diseñado para ser escalable, confiable y duradero. Se puede utilizar para una amplia gama de aplicaciones, como:\n",
    "\n",
    "* **Integración de datos:** Kafka se puede utilizar para recopilar datos de diferentes fuentes, como aplicaciones, sensores y dispositivos IoT.\n",
    "* **Análisis de datos:** Kafka se puede utilizar para almacenar datos para su análisis posterior.\n",
    "* **Notificación:** Kafka se puede utilizar para enviar notificaciones a los usuarios en tiempo real.\n",
    "\n",
    "Kafka funciona de la siguiente manera:\n",
    "\n",
    "* Los datos se envían a Kafka en forma de mensajes.\n",
    "* Kafka almacena los mensajes en particiones.\n",
    "* Los consumidores de datos pueden leer los mensajes de las particiones.\n",
    "\n",
    "Kafka utiliza un modelo de publicación-suscripción para enviar datos. Los productores de datos publican los datos en Kafka, y los consumidores de datos se suscriben a los topics de Kafka para recibir los datos.\n",
    "\n",
    "Kafka ofrece una serie de ventajas sobre otras plataformas de transmisión de datos, como:\n",
    "\n",
    "* **Escalabilidad:** Kafka es escalable horizontalmente, lo que significa que se puede agregar más nodos a un clúster de Kafka para aumentar el rendimiento.\n",
    "* **Fiabilidad:** Kafka está diseñado para ser confiable, incluso en caso de fallos de hardware o software.\n",
    "* **Durabilidad:** Kafka almacena los datos de forma duradera, lo que significa que los datos no se pierden en caso de fallos.\n",
    "\n",
    "Apache Kafka es una plataforma de transmisión de datos poderosa y flexible que se puede utilizar para una amplia gama de aplicaciones.\n",
    "\n",
    "En el contexto de Amazon Managed Streaming for Apache Kafka (MSK), los conceptos de \"broker node\" y \"Zookeeper node\" están relacionados con la infraestructura y el funcionamiento interno del clúster de Apache Kafka gestionado por MSK.\n",
    "\n",
    "1. **Broker Node:**\n",
    "   - **Definición:** Un \"broker node\" es un nodo individual en un clúster de Apache Kafka que actúa como servidor Kafka. Cada broker es responsable de almacenar datos y manejar las operaciones de productores y consumidores de Kafka.\n",
    "   - **Función:** Los broker nodes son esenciales para la distribución de mensajes en un clúster Kafka. Los productores envían mensajes a los broker nodes, y los consumidores los leen desde estos nodos. Cada broker en el clúster tiene una parte del conjunto total de datos.\n",
    "\n",
    "2. **Zookeeper Node:**\n",
    "   - **Definición:** Apache ZooKeeper es una herramienta de coordinación distribuida que a menudo se utiliza en conjunto con Apache Kafka para gestionar y coordinar el clúster Kafka.\n",
    "   - **Función:** En el caso de MSK, ZooKeeper se utiliza para gestionar tareas como la elección del líder de partición, la detección de fallos y la sincronización de configuraciones del clúster. Cada clúster de MSK tiene un conjunto de nodos ZooKeeper que son gestionados internamente por AWS.\n",
    "\n",
    "En clústeres de Kafka tradicionales, ZooKeeper también se usa para gestionar información sobre brokers y otros aspectos de la configuración del clúster. Sin embargo, con la versión más reciente de Apache Kafka y MSK, hay planes para eliminar gradualmente la dependencia de ZooKeeper para simplificar la arquitectura.\n",
    "\n",
    "Es importante destacar que la gestión de nodos y tareas internas, como la elección del líder de partición, puede ser gestionada completamente por el servicio MSK, y los usuarios no necesitan interactuar directamente con los nodos de ZooKeeper en la mayoría de los casos.\n",
    "\n",
    "Ten en cuenta que la información proporcionada aquí puede cambiar con el tiempo, ya que los servicios y las tecnologías evolucionan. Es aconsejable consultar la documentación oficial de MSK y Apache Kafka para obtener detalles actualizados sobre la arquitectura y el funcionamiento interno.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Arquitectura de MSK:\n",
    "\n",
    "En la parte superior vemos un cluster compuesto por 3 nodos desplegados en 3 sub-redes, desplegados en 3 zonas de disponibilidad, este cluster se conecta a unos nodos de Zookeeper. Tenemos el Producer de información alimentando al Leader en Apache Kafka, esta información es procesada y orquestada para luego ser enviada a los Consumers\n",
    "\n",
    "_ \n",
    "Los conceptos que manejamos en esta arquitectura son muy parecidos a los que tenemos en Kinesis Data Stream (un Producer, el Clúster, y un Consumer).\n",
    "\n",
    "![](img_98.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe988c10",
   "metadata": {},
   "source": [
    "## <a name=\"mark_18\"></a>Demo - Despliegue de un clúster con MSK\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Dentro de la consola de AWS --> MSK --> Create Cluster, creamos el nombre, elegimos la VPC (para el caso de la VPC por defecto, en este caso en Virginia, tiene 6 zonas de disponibilidad), y elegimos la versión de Kafka.\n",
    "\n",
    "![](img_99.png)\n",
    "\n",
    "Configuración de las zonas de disponibilidad, como mínimo 3 zonas de disponibilidad diferentes con sus sub-redes (las sub-redes están asociadas a las zonas de disponibilidad).\n",
    "\n",
    "![](img_100.png)\n",
    "\n",
    "Especificar la cantidad de Brokers, como vemos se multiplican por las zona de disponibilidad.\n",
    "\n",
    "![](img_101.png)\n",
    "\n",
    "Settings avanzados:\n",
    "\n",
    "![](img_102.png)\n",
    "\n",
    "\"Create Cluster\"\n",
    "\n",
    "![](img_103.png)\n",
    "\n",
    "Ya tenemos creado nuestro MSK, luego abrimos un IDE en Cloud9 para obtener mayor información del cluster y las cadenas de conexión.\n",
    "\n",
    "![](img_104.png)\n",
    "\n",
    "Comando a utilizar:\n",
    "\n",
    "`aws kafka list-clusters --region us-east-1` Lista los clusters en la región de Virginia, los datos que nos serán de utilidad son el ARN y su ZookeeperConnectString es la cadena de conexión que nos permitirá ingresar a Zookeeper y comenzar a orquestar las actividades del cluster.\n",
    "\n",
    "![](img_105.png)\n",
    "\n",
    "`aws kafka describe-cluster --region us-east-1 --cluster-arn ARN_cluster`, si tenemos más de un cluster por región al momento de listar los clusters, esta lista podría ser larga, con lo cual si queremos solamente información de uno solo cluster utilizamos esta sentencia + el ARN_cluster.\n",
    "\n",
    "![](img_106.png)\n",
    "\n",
    "Posteriormente nos falta una cadena de conexión, es la cadena de conexión de los Brokers, (no es la ZookeeperConnectString)\n",
    "\n",
    "`aws kafka get-bootstrap-brokers --region us-east-1 --cluster-arn ARN_cluster`\n",
    "\n",
    "![](img_107.png)\n",
    "\n",
    "Lo ejecutamos en la CLI, así recuperamos la cadena de conexión de nuestros Brokers (Recordar que tenemos un Broker por zona de disponibilidad).\n",
    "\n",
    "![](img_108.png)\n",
    "\n",
    "\n",
    "Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/msk/latest/developerguide/msk-list-clusters.html\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/msk/latest/developerguide/msk-get-connection-string.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f99bd",
   "metadata": {},
   "source": [
    "## <a name=\"mark_19\"></a>AWS - Glue\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Una vez la información en la nube procedemos al proceso de transformación para dejar la data lista para ser consumida por otros servicios.\n",
    "\n",
    "Glue es un servicio de ETL totalmente administrado con lo cual no vamos a tener acceso a un cluster, a un sistema operativo, configuraremos las tareas que requerimos, las enviaremos a Glue, y el servicio solamente cobrará por el tiempo de ejecución de esas tareas y por las unidades que usa de capacidad para la ejecución de las tareas.\n",
    "\n",
    "El catálago es muy importante ya que alimenta a otros servicios como Athena, el catálogo es una metadata el cual nos dice donde está la información que necesitamos, con lo cual otros servicios se conectan al catálogo hacen la consulta, el catálogo dice donde está la información y la llevamos de vuelta al servicio que estamos consultando.\n",
    "\n",
    "![](img_109.png)\n",
    "\n",
    "Glue utiliza unidades de capacidad llamadas DPU (Data Processing Units), la mínima que podemos utilizar (para hacer las pruebas en los developer endpoints) son 2 DPU, y 1 DPU = 4 vCPU y 16GB RAM. Con lo cual la cantidad de DPU se incrementará con el incremento del tiempo de ejecución del job, aumentando el precio.\n",
    "\n",
    "Glue Catalog, será el almacen de metadatos el cual puede ser consultado por otros servicios para acceder a la raw data o a la data transformada, cada cuenta de AWS tiene por defecto 1 Glue Catalog.\n",
    "\n",
    "Los Crawlers va a ir a escanear la data, identificar la data y la ponen en el catálogo, y los Classifiers podemos especificar, por ejemplo cuando tenemos data en muchos formatos o en un JSON muy complejo, podemos ejemplificar en el Classifier un JSON de ejemplo para que el servicio lo tome y pueda identificar mejor la data origen y ponerla en el catálogo.\n",
    "\n",
    "![](img_110.png)\n",
    "\n",
    "### Como funciona:\n",
    "\n",
    "1. Los Crawlers que van a identificar la información origine para crear el catálogo.\n",
    "\n",
    "2. Glue como tal que va a correr el job de ETL, y vamos a alimentar el Glue Catalog.\n",
    "\n",
    "3. Glue se puede conectar a diferentes fuentes, DDBB relacionales, no relacionales, S3 almacenamiento por objetos y podemos conectarlo a otros servicios, él hace la trasformación de la información la pone en otro sistema de almacenamiento (usalmente un S3) y de ahí en adelante podemos alimentar diferentes servicios como Athena (Athena usa el Glue Catalog para hacer consultas SQL),  Redshift, EMR, o una cadena de transformación con otro Glue ETL (podemos hacer ETLs anidados y orquestados)\n",
    "\n",
    "![](img_111.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d5a69",
   "metadata": {},
   "source": [
    "## <a name=\"mark_20\"></a>Demo - Instalando Apache Zeppelin\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Apache Zeppelin tiene integración los servicios de AWS, se puede integrar directamente con Glue y lo podemos desplegar en clusters de EMR\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Amazon EMR (conocido antes como Amazon Elastic MapReduce) es una plataforma de clúster administrada que simplifica la ejecución de los marcos de trabajo de macrodatos, tales como Apache Hadoop y Apache Spark, en AWS para procesar y analizar grandes cantidades de datos. \n",
    "\n",
    "Mediante el uso de estos marcos de trabajo y proyectos de código abierto relacionados, puede procesar datos para fines de análisis y cargas de trabajo de inteligencia empresarial. \n",
    "\n",
    "Además, Amazon EMR le permite transformar y trasladar grandes cantidades de datos hacia y desde otros almacenes de datos y bases de datos de AWS, tales como Amazon Simple Storage Service (Amazon S3) y Amazon DynamoDB.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "![](img_112.png)\n",
    "\n",
    "### Instalando Apache Zeppelin en nuestro local:\n",
    "\n",
    "![](img_113.png)\n",
    "\n",
    "Dentro de las imagenes de Apache Zeppelin, hasta el momento AWS tiene compatibilidad con la siguiente versión, hacemos click sobre el enlace no redirige hacia Apache Software Fundation.\n",
    "\n",
    "![](img_114.png)\n",
    "\n",
    "![](img_115.png)\n",
    "\n",
    "Una vez realizado el download de archivo comprimido .tgz, lo descomprimimos en la carpeta que deseemos y mediante la CLI ingresamos a esa carpeta y lanzamos el servicio con el deamon de Zeppelin.\n",
    "\n",
    "![](img_116.png)\n",
    "\n",
    "Nos dirigimos a nuestro navegador y podremos ver el servicio ejecutado en el puerto 8080.\n",
    "\n",
    "![](img_117.png)\n",
    "\n",
    "![](img_118.png)\n",
    "\n",
    "![](img_119.png)\n",
    "\n",
    "\n",
    "### Sugerencia utilizando docker-compose.yml\n",
    "\n",
    "```yml\n",
    "version: \"3\"\n",
    "\n",
    "volumes:\n",
    "  zeppelin_notebook: {}\n",
    "  zeppelin_conf: {}\n",
    "\n",
    "services:\n",
    "  zeppelin:\n",
    "    image: apache/zeppelin:0.8.1\n",
    "    volumes:\n",
    "      - zeppelin_notebook:/zeppelin/notebook\n",
    "      - zeppelin_conf:/zeppelin/conf\n",
    "    ports:\n",
    "      - '8080:8080'\n",
    "```\n",
    "\n",
    "`docker-compose up`\n",
    "\n",
    "### Sugerencia utilizando docker:\n",
    "\n",
    "`docker pull apache/zeppelin:0.10.0`\n",
    "\n",
    "`docker run -p 8080:8080 -p 4040:4040 -p 4041:4041 -p 4042:4042 -v /zeppelin/notebook:/zeppelin/notebook --name zeppelin apache/zeppelin:0.10.0`\n",
    "\n",
    "Lectura Recomendada:\n",
    "\n",
    "https://zeppelin.apache.org/docs/0.8.1/quickstart/install.html#starting-apache-zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4c421",
   "metadata": {},
   "source": [
    "## <a name=\"mark_21\"></a>Creación del Developer Endpoint\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Vamos a configurara un developer endpoint dentro de AWS el cual será un puente para conectar nuestro Apache Zeppelin local y poder consultar data dentro de AWS.\n",
    "\n",
    "Dentro de la consola de AWS --> Glue --> Dev Endpoints --> Notebooks\n",
    "\n",
    "![](img_120.png)\n",
    "\n",
    "![](img_121.png)\n",
    "\n",
    "![](img_122.png)\n",
    "\n",
    "Creamos un endpoint completamente nuevo, el role utilizado debe tener permisos sobre todos los servicios de Glue y S3 en el cual vamos a tener nuestra data.\n",
    "\n",
    "![](img_123.png)\n",
    "\n",
    "Podemos cargar librerías que necesitemos, Python o jar cargandolas desde un S3, los DPU quedan en 5.\n",
    "\n",
    "![](img_124.png)\n",
    "\n",
    "Elegimos el tipo de conexión generico, pero también en el caso de que tengamos una arquitectura con VPC, Sub-redes y grupos de seguridad podremos lanzar el endpoint en esos ambientes.\n",
    "\n",
    "![](img_125.png)\n",
    "\n",
    "Nos pedirá una llave, esta llave la creamos localmente y solamente cargamos la llave pública.\n",
    "\n",
    "![](img_126.png)\n",
    "\n",
    "Creamos la llave en el local `ssh-keygen`, le colocamos un nombre y un passphrase si queremos.\n",
    "\n",
    "![](img_127.png)\n",
    "\n",
    "![](img_128.png)\n",
    "\n",
    "Abrimos platzillave.pub con `vim platzillave.pub` y copiamos la llave publica.\n",
    "\n",
    "![](img_129.png)\n",
    "\n",
    "Cargamos la llave publica al developer endpoint\n",
    "\n",
    "![](img_130.png)\n",
    "\n",
    "![](img_131.png)\n",
    "\n",
    "Click en Finish, en este momento comienza una fase de aprovisonamiento de nuestro developer endpoint\n",
    "\n",
    "### Importante, el developer endpoint ya no está disponible de esta forma.\n",
    "\n",
    "OFICIALMENTE Dev Endpoints obsoletos https://docs.aws.amazon.com/glue/latest/dg/development.html\n",
    "\n",
    "Console experience for dev endpoints will be removed starting March 31, 2023. Following this date, creating, updating, and monitoring dev endpoints will be available via the AWS Glue API and AWS Glue CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df13f7",
   "metadata": {},
   "source": [
    "## <a name=\"mark_22\"></a>Demo - Conectando nuestro developer Endpoint a nuestro Zeppelin Edpoint\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "![](img_132.png)\n",
    "\n",
    "Ya tenemos listo el developer endpoint, nos vamos a nuestro Apache Zeppelin para realizar las configuración que permitiran que se pueda comunicar con el developer endpoint.\n",
    "\n",
    "![](img_133.png)\n",
    "\n",
    "![](img_134.png)\n",
    "\n",
    "Según la documentación deberíamos setearlo con las siguientes características.\n",
    "\n",
    "![](img_135.png)\n",
    "\n",
    "Entonces en Properties --> yarn-client\n",
    "\n",
    "Si hay/existe spark.executor.memory --> eliminamos esa propiesdad con la X\n",
    "\n",
    "Si hay/existe spark.driver.memory --> eliminamos esa propiesdad con la X\n",
    "\n",
    "![](img_136.png)\n",
    "\n",
    "Configuramos el re-direct (Connect to existing process) hacia el puerto 9007\n",
    "\n",
    "![](img_137.png)\n",
    "\n",
    "Guardamos los cambios y estamos listo para conectarnos a nuestro developer endpoint\n",
    "\n",
    "Dentro de nuestro endpoint copiamos el valor de \"SSH to Python RELP\"\n",
    "\n",
    "![](img_138.png)\n",
    "\n",
    "REPL significa Read-Evaluate-Print Loop, y es un tipo de interfaz de usuario que permite a los usuarios interactuar con un lenguaje de programación. En un REPL, los usuarios pueden ingresar código, que se evalúa y se imprime el resultado.\n",
    "\n",
    "Nos vamos a nuestra local y en la carpeta donde tenemos la llave privada ejecutamos el comando copiado, reemplazando `<private-key.pem>` por nuestra llave privada (poner el nombre sin el .pem).\n",
    "\n",
    "![](img_139.png)\n",
    "\n",
    "![](img_140.png)\n",
    "\n",
    "Listo conectados con Python y Spark, podemos hacer lo mismo con Scala y Spark y utilizando el \"SSH Scala to REPL\"\n",
    "\n",
    "Bueno ahora debemos crear el tunel que le permita a mi zeppeling utilizar los datos en AWS, para eso copiamos el \"SSH tunnel to remote interprete\" y lo ejecutamos en nuestro local reemplazando la `private-key.pem` por nuestra llave privada.\n",
    "\n",
    "![](img_141.png)\n",
    "\n",
    "Este proceso correrá en un loop infinito (hasta que lo detengamos) desde nuestro developer endpoint a nuestro Zeppelin (maneja la conexión entre ambos), ahora sí podemos crear un Notebook y comenzar a interactura con AWS.\n",
    "\n",
    "![](img_142.png)\n",
    "\n",
    "Lecturas Recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-local-notebook.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4a0d3",
   "metadata": {},
   "source": [
    "## <a name=\"mark_23\"></a>Demo - Creando nuestro primer ETL - Crawling\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "El Crawler es una tarea que irá a nuestro bucket identificará la data y la pondrá en el Blue Catalog (Catálog de metadatos persistente), este Catalog tendrá información sobre la data su estructura y donde esta almacenada para que otros servicios utilicen ese catálogo y puedan acceder a esa información.\n",
    "\n",
    "Necesitaremos un Bucket origen \"origen-platzi\" y un bucket destino \"target-platzi\", para este ejemplo usamos S3, entonces en el bucket origen tendremos la data cruda y el bucket destino la data ya transformada.\n",
    "\n",
    "Nuestro bucket con la data cruda.\n",
    "\n",
    "![](img_143.png)\n",
    "\n",
    "Nos vamos al Servicio de Glue y agregamos un nuevo Crawler\n",
    "\n",
    "![](img_144.png)\n",
    "\n",
    "Colocamos un nombre, a nivel de seguridad no tenemos opciones para este ejemplo \"none\", podemos agregarle classifiers que serviran para identificar la estructura de la data en el origen y poder leerla correctamente.\n",
    "\n",
    "![](img_145.png)\n",
    "\n",
    "Podemos agrupar para que se pueda leer desde multiples S3.\n",
    "\n",
    "![](img_146.png)\n",
    "\n",
    "Elegimos el origen de la data (S3, JDBC, o DynamoDB), JDBC significa Java Database Connectivity. Es un API que permite a los desarrolladores de Java acceder a bases de datos relacionales.\n",
    "\n",
    "![](img_147.png)\n",
    "\n",
    "Exclute patterns (optional), si queremos excluir algún patrón.\n",
    "\n",
    "![](img_148.png)\n",
    "\n",
    "![](img_149.png)\n",
    "\n",
    "![](img_150.png)\n",
    "\n",
    "Podemos dejarlo en \"on demand\".\n",
    "\n",
    "![](img_151.png)\n",
    "\n",
    "Ok si vamos a realizar el Crawling de la data, en que db la vamos colocar.\n",
    "\n",
    "![](img_152.png)\n",
    "\n",
    "![](img_153.png)\n",
    "\n",
    "![](img_154.png)\n",
    "\n",
    "![](img_155.png)\n",
    "\n",
    "![](img_156.png)\n",
    "\n",
    "Al correr el Crawler, el ira al bucket de origen va a identificar todos los JSON, su estructura, y se lleva toda esa información a Glue Catalog, y en el Glue Catalog toda esa información va a quedar debajo de la db que creamos, creando cada JSON como una tabla.\n",
    "\n",
    "![](img_157.png)\n",
    "\n",
    "![](img_158.png)\n",
    "\n",
    "![](img_159.png)\n",
    "\n",
    "![](img_160.png)\n",
    "\n",
    "Como el Glue Catalog es utilizado por otros servicios, podemos por ejemplo ir al servicio de Athena y buscar nuestra db.\n",
    "\n",
    "![](img_161.png)\n",
    "\n",
    "![](img_162.png)\n",
    "\n",
    "Las tablas estan pero no estan ordendas, porque es data cruda, entonce ahí entraremos con la transformación.\n",
    "\n",
    "En definitiva Athena consulta el Glue Caltalog, que le indica que la data está en S3 y allí irá a buscar la data.\n",
    "\n",
    "Lectura Recomendada:\n",
    "\n",
    "https://docs.aws.amazon.com/es_es/glue/latest/dg/aws-glue-programming-python-samples-legislators.html\n",
    "\n",
    "https://github.com/czam01/glue-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90275cac",
   "metadata": {},
   "source": [
    "## <a name=\"mark_24\"></a>Demo - Creando nuestro primer ETL - Ejecución\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Dentro del github.com/aws-samples (ver \"Lecturas Recomendadas\") encontraremos la documentación que nos permite por ejemplo Crear el Glue Context, consultar su schema y demás.\n",
    "\n",
    "![](img_163.png)\n",
    "\n",
    "### Creamos el Context.\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import Join\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "```\n",
    "\n",
    "### Comprobación de los esquemas que identificó el rastreador\n",
    "\n",
    "```python\n",
    "persons = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"persons_json\")\n",
    "print(\"Count: \", persons.count())\n",
    "persons.printSchema()\n",
    "```\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "**1. Creación de un DynamicFrame:**\n",
    "\n",
    "```python\n",
    "persons = glueContext.create_dynamic_frame.from_catalog(database=\"legislators\", table_name=\"persons_json\")\n",
    "```\n",
    "\n",
    "- **glueContext:** Es un objeto que proporciona acceso a las funcionalidades de Glue.\n",
    "- **create_dynamic_frame.from_catalog():** Es un método que crea un DynamicFrame a partir de una tabla en el catálogo de datos de Glue.\n",
    "- **database=\"legislators\":** Indica la base de datos donde se encuentra la tabla.\n",
    "- **table_name=\"persons_json\":** Indica el nombre de la tabla que contiene los datos.\n",
    "\n",
    "Esta línea crea un DynamicFrame llamado `persons` que contiene los datos de la tabla `persons_json` en la base de datos `legislators`. Un DynamicFrame es una estructura de datos flexible que se utiliza en Glue para representar datos estructurados y semiestructurados.\n",
    "\n",
    "**2. Imprimir el número de registros:**\n",
    "\n",
    "```python\n",
    "print(\"Count: \", persons.count())\n",
    "```\n",
    "\n",
    "- **persons.count():** Cuenta el número de registros en el DynamicFrame `persons`.\n",
    "- **print(\"Count: \", ...):** Imprime el número de registros en la consola.\n",
    "\n",
    "Esta línea imprime la cantidad de registros que se encuentran en el DynamicFrame `persons`.\n",
    "\n",
    "**3. Imprimir el esquema del DynamicFrame:**\n",
    "\n",
    "```python\n",
    "persons.printSchema()\n",
    "```\n",
    "\n",
    "- **persons.printSchema():** Imprime la estructura del esquema del DynamicFrame `persons`. El esquema muestra los nombres de las columnas y sus tipos de datos.\n",
    "\n",
    "Esta línea imprime en la consola la estructura del esquema del DynamicFrame, permitiendo observar la organización de los datos.\n",
    "\n",
    "**En resumen:**\n",
    "\n",
    "El código importa datos desde una tabla en Glue, obtiene la cantidad de registros y muestra la estructura del esquema de los datos. Esto permite comprender la información contenida en el DynamicFrame y sus características.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Notas_01:\n",
    "\n",
    "Jupyter Notebook es una interfaz generada por AWS, es muy simple configurar, mucho mas que zepellin.\n",
    "\n",
    "Documentación https://docs.aws.amazon.com/es_es/glue/latest/dg/dev-endpoint-tutorial-sage.htm\n",
    "\n",
    "### Lecturas recomendadas:\n",
    "\n",
    "https://github.com/aws-samples/aws-glue-samples/blob/master/examples/join_and_relationalize.md\n",
    "\n",
    "https://github.com/czam01/glue-examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def470c",
   "metadata": {},
   "source": [
    "## <a name=\"mark_25\"></a>Demo - Creando nuestro primer ETL - Carga\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Vamos a Glue --> Jobs --> Add Job, agregaremos un trabajo, el cual realizará las transformaciones.\n",
    "\n",
    "![](img_164.png)\n",
    "\n",
    "![](img_165.png)\n",
    "\n",
    "![](img_166.png)\n",
    "\n",
    "![](img_167.png)\n",
    "\n",
    "![](img_168.png)\n",
    "\n",
    "![](img_169.png)\n",
    "\n",
    "![](img_170.png)\n",
    "\n",
    "Damos en crear y editar, en esta parte colocaremos nuestro script etl.py.\n",
    "\n",
    "![](img_171.png)\n",
    "\n",
    "### etl.py \n",
    "\n",
    "```python\n",
    "# Librerias\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "# catalog: bases de datos y sys nombres (nombres de las tablas)\n",
    "db_name = \"platzidb\"\n",
    "tbl_persons = \"persons_json\"\n",
    "tbl_membership = \"memberships_json\"\n",
    "tbl_organization = \"organizations_json\"\n",
    "\n",
    "# Directorios de salida enS3 \"nuestro target\"\n",
    "output_history_dir = \"s3://target-platzi/legislator_history\"\n",
    "output_lg_single_dir = \"s3://target-platzi/legislator_single\"\n",
    "output_lg_partitioned_dir = \"s3://target-platzi/legislator_part\"\n",
    "\n",
    "# Creacion de los Dynamic Frames de las 3 tablas de origen\n",
    "'''\n",
    "Dentro de la transformación vamos a tener 3 tablas, dentro de esas tablas vamos a crear el dynamic frame, le vamos a decir a Glue que empiece a identificar esos origenes, para que con el dynamic frame empiece a gestionar la transformación\n",
    "'''\n",
    "persons = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=tbl_persons)\n",
    "memberships = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=tbl_membership)\n",
    "orgs = glueContext.create_dynamic_frame.from_catalog(database=db_name, table_name=tbl_organization)\n",
    "\n",
    "# Eliminamos \"other_names\" e \"identifiers\" y se renombramos \"id\" y \"name\".\n",
    "orgs = orgs.drop_fields(['other_names', 'identifiers']).rename_field('id', 'org_id').rename_field('name', 'org_name')\n",
    "\n",
    "# Join de los frames para crear una sola tabla\n",
    "l_history = Join.apply(orgs, Join.apply(persons, memberships, 'id', 'person_id'), 'org_id', 'organization_id').drop_fields(['person_id', 'org_id'])\n",
    "\n",
    "# ---- Escribiendo las 3 salidas de la historia ----\n",
    "\n",
    "# escribiendo el dynamic frame en formato parquet en el directorio \"legislator_history\" \n",
    "glueContext.write_dynamic_frame.from_options(frame = l_history, connection_type = \"s3\", connection_options = {\"path\": output_history_dir}, format = \"parquet\")\n",
    "\n",
    "# Escribiendo un simple archivo en el directorio \"legislator_single\"\n",
    "s_history = l_history.toDF().repartition(1)\n",
    "s_history.write.parquet(output_lg_single_dir)\n",
    "\n",
    "# Convirtiendo a dataframe, escribiendo en el directorio \"legislator_part\"\n",
    "l_history.toDF().write.parquet(output_lg_partitioned_dir, partitionBy=['org_name'])\n",
    "```\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "Para la siguiente parte de un código, las opciones A, B, y C son equivalentes? explicar partitionBy=['org_name']\n",
    "\n",
    "# A\n",
    "glueContext.write_dynamic_frame.from_options(frame = l_history, connection_type = \"s3\", connection_options = {\"path\": output_history_dir}, format = \"parquet\")\n",
    "\n",
    "# B\n",
    "s_history = l_history.toDF().repartition(1)\n",
    "s_history.write.parquet(output_history_dir)\n",
    "\n",
    "# C\n",
    "l_history.toDF().write.parquet(output_history_dir, partitionBy=['org_name'])\n",
    "\n",
    "**No, las opciones A, B y C no son totalmente equivalentes.** Aunque todas escriben un DynamicFrame en formato Parquet en S3, tienen diferencias importantes:\n",
    "\n",
    "**Opción A:**\n",
    "\n",
    "- Utiliza la función `write_dynamic_frame.from_options` de Glue para escribir el DynamicFrame directamente a S3.\n",
    "- No realiza particiones de los datos.\n",
    "- Ofrece un mayor control sobre las opciones de escritura a través de los parámetros `connection_type`, `connection_options` y `format`.\n",
    "\n",
    "**Opción B:**\n",
    "\n",
    "- Convierte el DynamicFrame a un DataFrame de Spark usando `toDF()`.\n",
    "- Redistribuye los datos en una sola partición usando `repartition(1)`.\n",
    "- Escribe el DataFrame en formato Parquet usando la función `write.parquet` de Spark.\n",
    "- No realiza particiones de los datos.\n",
    "- Podría generar un solo archivo Parquet muy grande.\n",
    "\n",
    "**Opción C:**\n",
    "\n",
    "- Convierte el DynamicFrame a un DataFrame de Spark usando `toDF()`.\n",
    "- Escribe el DataFrame en formato Parquet usando la función `write.parquet` de Spark.\n",
    "- Particiona los datos por la columna `org_name`, creando subdirectorios en S3 con base en los valores de esa columna.\n",
    "- Permite consultas más eficientes sobre los datos particionados.\n",
    "\n",
    "**Explicación de `partitionBy=['org_name']`:**\n",
    "\n",
    "- Es un parámetro de la función `write.parquet` que indica cómo se particionarán los datos.\n",
    "- En este caso, los datos se dividirán en subdirectorios en S3 según los valores de la columna `org_name`.\n",
    "- Esto permite optimizar las consultas que filtren por la columna `org_name`, ya que solo se necesitará leer los datos del subdirectorio correspondiente.\n",
    "\n",
    "En resumen:\n",
    "\n",
    "- La opción A es la más adecuada si se desea un mayor control sobre las opciones de escritura y no se requiere particionar los datos.\n",
    "- La opción C es la más adecuada si se desea particionar los datos por la columna `org_name` para optimizar consultas posteriores.\n",
    "- La opción B, al generar un solo archivo, podría ser útil en casos específicos, pero generalmente no es la opción más recomendada para grandes conjuntos de datos.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "![](img_172.png)\n",
    "\n",
    "![](img_173.png)\n",
    "\n",
    "Para este ejemplo dejamos todo como está y corremos el job.\n",
    "\n",
    "![](img_174.png)\n",
    "\n",
    "Si vamos a nuestro S3 target, veremos las carpetas creadas y los archivos transforrmados.\n",
    "\n",
    "![](img_175.png)\n",
    "\n",
    "Ahora como consumimos esa data transformada --> creamos un nuevo Crawler pero esta vez que apunte al S3 target para que identifique esta nueva data.\n",
    "\n",
    "![](img_176.png)\n",
    "\n",
    "Creamos una nueba db.\n",
    "\n",
    "![](img_177.png)\n",
    "\n",
    "![](img_178.png)\n",
    "\n",
    "Luego como ya vimos, podemos ver las tablas agregadas en Glue --> Data Catalog --> Databases --> Tables\n",
    "\n",
    "![](img_179.png)\n",
    "\n",
    "De igual forma consultando el Data Catalog desde Athena.\n",
    "\n",
    "![](img_181.png)\n",
    "\n",
    "![](img_180.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602d23a",
   "metadata": {},
   "source": [
    "## <a name=\"mark_26\"></a>AWS - EMR - Elastic Map Reduce\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "El nombre de AWS EMR, Elastic MapReduce, se debe a dos conceptos clave:\n",
    "\n",
    "**Elasticidad**: La elasticidad es una característica importante de EMR, ya que permite adaptar los recursos del clúster a las necesidades del trabajo. Esto se hace a través de un proceso llamado autoescalamiento.\n",
    "\n",
    "El autoescalamiento de EMR funciona de la siguiente manera:\n",
    "\n",
    "    _ EMR monitorea el uso de los recursos del clúster.\n",
    "    _ Si el uso de los recursos alcanza un umbral determinado, EMR escala el clúster hacia arriba.\n",
    "    _ Si el uso de los recursos disminuye, EMR escala el clúster hacia abajo.\n",
    "\n",
    "Garantiza que tener los recursos necesarios para procesar los datos, sin tener que preocuparse por configurar y administrar el clúster manualmente.\n",
    "\n",
    "**MapReduce**: Utiliza el procesamiento de datos distribuidos. MapReduce es un modelo muy eficiente para procesar grandes conjuntos de datos, ya que divide el trabajo en tareas más pequeñas que pueden ejecutarse de forma paralela en múltiples nodos. MapReduce es un modelo muy eficiente para procesar grandes conjuntos de datos.\n",
    "\n",
    "Aquí están algunos puntos clave sobre EMR:\n",
    "\n",
    "    - Simplicidad y escalabilidad: Te permite lanzar clústeres de procesamiento con solo unos clics, y escalarlos hacia arriba o hacia abajo según tus necesidades.\n",
    "\n",
    "    - Soporte para múltiples frameworks: Puedes elegir entre frameworks populares como Hadoop, Spark, Flink, Presto y HBase para procesar tus datos.\n",
    "\n",
    "    - Costos optimizados: Solo pagas por los recursos que utilizas, gracias al modelo de pago por uso de AWS.\n",
    "\n",
    "    - Integración con otros servicios de AWS: EMR se integra fácilmente con otros servicios de AWS como S3 (almacenamiento), DynamoDB (base de datos) y Redshift (almacén de datos) para un flujo de trabajo de Big Data completo.\n",
    "\n",
    "¿Para qué se utiliza EMR?\n",
    "\n",
    "EMR se utiliza para una amplia variedad de tareas de Big Data, incluyendo:\n",
    "\n",
    "    - Análisis de logs y de eventos: Procesamiento de grandes volúmenes de datos de aplicaciones, servidores y sensores.\n",
    "    \n",
    "    - Machine learning: Entrenamiento de modelos de aprendizaje automático a partir de grandes conjuntos de datos.\n",
    "\n",
    "    - Procesamiento de datos científicos: Análisis de datos genómicos, astronómicos y de otras ciencias.\n",
    "\n",
    "    - Ingesta y ETL de datos: Carga y transformación de datos de diversas fuentes en almacenes de datos.\n",
    "\n",
    "¿Cuáles son las ventajas de usar EMR?\n",
    "\n",
    "    - Reduce la complejidad de administrar clústeres de Big Data: No necesitas preocuparte por configurar y mantener la infraestructura del clúster.\n",
    "\n",
    "    - Acelera el procesamiento de datos: EMR proporciona recursos informáticos de alto rendimiento para procesar grandes conjuntos de datos rápidamente.\n",
    "\n",
    "    - Te permite centrarte en tu análisis: Puedes centrarte en el análisis de tus datos sin tener que preocuparte por la infraestructura subyacente.\n",
    "\n",
    "    - Es rentable: Solo pagas por los recursos que utilizas.\n",
    "    \n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Cuando hablemos de EMR, pensemos en un clúster en el cual podemos correr cargas de trabajo muy grandes; es obvio que al ser un clúster tendremos unas instancias dedicadas, mayor administración sobre nuestro clúster pero mayor flexibilidad.\n",
    "\n",
    "![](img_182.png)\n",
    "\n",
    "EMR es un servicio que nos permite crear clústers que por detrás serán instancias EC2 basadas en Hadoop; en estos clústers podremos ejecutar diferentes cargas de trabajo.\n",
    "\n",
    "Estas cargas de trabajo pueden ser MapReduce, Spark, Pig, Presto, Hive, Impala, Flink, TensorFlow, Zeppelin, diferentes y muchas alternativas de Open Source pueden correr en estos proyectos, y cuando configuremos el clúster podremos seleccionar cuáles de ellos necesitamos.\n",
    "\n",
    "Este servicio nos provee integraciones con diferentes servicios de AWS como S3, RedShift, DynamoDB y Kinesis.\n",
    "\n",
    "![](img_183.png)\n",
    "\n",
    "En el clúster desplegado por este servicio, podemos correr cualquier tipo de tareas basadas en Spark; es decir, podríamos correr los ETLs que hacemos en Glue, pero cambiarían algunas cosas en la configuración debido a que Glue utiliza **Dynamic Frames** (son más propios de AWS para manejar Glue) mientras que EMR usa otro concepto que es **Data Frames** que es más propio de Spark. Habiendo dicho esto, podríamos también hacer aquí transformaciones, y podríamos conectarnos a Notebooks para consultar nuestra información desde EMR.\n",
    "\n",
    "### Conceptos\n",
    "\n",
    "_ Bootstrap actions (acciones de arranque): \n",
    "\n",
    "Al momento de desplegar el clúster, podemos especificar ciertas acciones que necesitemos que se ejecuten al inicio; acciones como ejecuciones de scripts particulares, cambiar los puertos de conexión, o agregarle configuraciones muy personalizadas al clúster. Estas tareas se ejecutan antes de que el clúster esté completamente productivo.\n",
    "\n",
    "_ Steps: Nos permiten ejecutar nuestras cargas de trabajo en los clústers de EMR de forma ordenada.\n",
    "\n",
    "_ Clúster: Está compuesto de un Master Node, quien orquestará a los otros Nodes; Core Nodes que se encargan de distribuir la información cuando usamos un sistema de archivos como HDFS (Sistema de archivos distribuido), y Task Nodes que son los encargados del procesamiento.\n",
    "\n",
    "![](img_184.png)\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "A nivel de instancias, utilizar instancias tipo Spot para los Core Nodes y los Task Nodes, recordar que las instancias tipo Spot son por subastas.\n",
    "\n",
    "El Master Node no debe ser desplegado en una instancia Spot, debido a que como son instancias por subasta, podrían dejarnos sin Master (si algún ofrece un bit más alto se cae nuestro cluster), con lo cual durante el seteo de EMR especificaremos que si no hay instancias Spot tome instancias por demanda, el pricing es mayor pero no nos quedamos sin recursos.\n",
    "\n",
    "Es así como EMR es un servicio muy importante donde podemos correr cargas de trabajo gigantescas, porque tenemos la libertad de crear las instancias, elegir el tipo y la cantidad.\n",
    "\n",
    "En situaciones como que en Glue hay un límite de 100 DPUs, pero nuestro job es muy grande, AWS recomienda dividir nuestro job en jobs más pequeños para no consumir tantas DPUs o trabajar con EMR en donde tenemos más capacidad pero nuestra carga de información y nuestra carga administrativa aumentará."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acadf0d0",
   "metadata": {},
   "source": [
    "## <a name=\"mark_27\"></a>Demo - Desplegando nuestro primer clúster con EMR\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Dentro de la consola de AWS buscamos EMR --> Crear Cluster\n",
    "\n",
    "![](img_185.png)\n",
    "\n",
    "![](img_186.png)\n",
    "\n",
    "![](img_187.png)\n",
    "\n",
    "Opcionales\n",
    "\n",
    "![](img_188.png)\n",
    "\n",
    "![](img_189.png)\n",
    "\n",
    "![](img_190.png)\n",
    "\n",
    "Si agregamos una aplicación de Spark y hacemos click en \"Configurar\"\n",
    "\n",
    "Argumentos --> Esta aplicación, ¿con qué argumentos va a ser lanzada?  (PRD: Producción, STG: Staging, etc)\n",
    "\n",
    "Ubicación de la aplicación --> desde dónde la llamaremos.\n",
    "\n",
    "Acciones sobre el error --> Terminar Cluster (si el step terminó, termine el clúster), Cancelar, esperar (por una acción manual), Continuar (falló el step, continue con el siguiente)\n",
    "\n",
    "![](img_191.png)\n",
    "\n",
    "Para este ejemplo cancelamos \"Añadir pasos\" y damos en \"Next\" en las \"Configuraciones Avanzadas\"\n",
    "\n",
    "![](img_192.png)\n",
    "\n",
    "![](img_193.png)\n",
    "\n",
    "![](img_194.png)\n",
    "\n",
    "Configurando el Autoscaling\n",
    "\n",
    "![](img_195.png)\n",
    "\n",
    "Para modificar el autoscaling damos click en editar, luego configuraremos las reglas de escalamiento dependiendo nuestro tipo de job, porque dependiendo el tipo de job es la métrica en la que vamos a crecer nuestra tarea, por ejemplo lo podemos hacer por memoria disponible.\n",
    "\n",
    "![](img_196.png)\n",
    "\n",
    "Lo mismo para desescalar.\n",
    "\n",
    "![](img_197.png)\n",
    "\n",
    "Configuración general del cluster:\n",
    "\n",
    "![](img_198.png)\n",
    "\n",
    "![](img_199.png)\n",
    "\n",
    "![](img_200.png)\n",
    "\n",
    "Opciones de seguridad: Podemos seleccionar una llave de conexión a nuestros cluster, o si no queremos conectarnos lo dejamos en \"Continuar sin un par de claves EC2\", la diferencia radica que al momento de lanzar un cluster podemos hacer un tunel SSH y empezar a mirar con mayor detalle, tareas o ejecuciones de las tareas, o correrlas manualmente dentro del cluster, si no especificamos una llave la ejecución se realiza sin mayores detalles.\n",
    "\n",
    "Los permisos, para este ejemplo quedarán predeterminados, el sistema crea 3 tipos de errores\n",
    "\n",
    "![](img_201.png)\n",
    "\n",
    "Para este ejemplo no vamos a configurar \"seguridad\", pero sepamos que se integra con KMS.\n",
    "\n",
    "![](img_202.png)\n",
    "\n",
    "Y podemos crear grupos de seguridad, las podemos personalizar, para por ejemplo espcificar si la instancia Maestro necesitar algún tipo de puerto extra, o cerraremos más puertos, y lo mismo para las otras instancias, para este caso lo dejamos como viene por defecto y damos click en \"crear cluster\".\n",
    "\n",
    "![](img_203.png)\n",
    "\n",
    "Ya creado nuestro clúster abrimos el cluster.\n",
    "\n",
    "![](img_204.png)\n",
    "\n",
    "Para realizar la conexión, debemos habilitar en el security group de esa conexión.\n",
    "\n",
    "![](img_205.png)\n",
    "\n",
    "![](img_206.png)\n",
    "\n",
    "![](img_207.png)\n",
    "\n",
    "![](img_208.png)\n",
    "\n",
    "Añadir paso.\n",
    "\n",
    "![](img_209.png)\n",
    "\n",
    "![](img_210.png)\n",
    "\n",
    "Si vemos en la parte superior tenemos \"Exportación de la CLI de AWS\", esto arroja un comando el cual podemos utilizar si necesitamos generar nuevamente el cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3059723",
   "metadata": {},
   "source": [
    "## <a name=\"mark_28\"></a>Demo - Conectándonos a Apache Zeppelin en EMR\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "Como vimos anteriormente, no tenemos la conexión web habilitada, debemos abrir esta conexión en los grupos de seguridad desde Master node. Depende de las herramientas que hayamos instalado en el servidor serán los puertos que debemos habilitar.\n",
    "\n",
    "Diferentes puertos para diferentes tipos de servicios.\n",
    "\n",
    "![](img_211.png)\n",
    "\n",
    "Según como vemos en la imagen, para Zeppelin debemos abrir el \"master-public-dns-name:8890\"\n",
    "\n",
    "![](img_212.png)\n",
    "\n",
    "Nos redirecciona a los grupos de seguridad.\n",
    "\n",
    "![](img_213.png)\n",
    "\n",
    "![](img_214.png)\n",
    "\n",
    "![](img_215.png)\n",
    "\n",
    "![](img_216.png)\n",
    "\n",
    "![](img_217.png)\n",
    "\n",
    "Dentro de las mejores prácticas de seguridad, ejecutamos el cluster de EMR en una Sub-red privada y en la sub-red publica ponemos un balanceador de carga, luego un certificado de seguridad por Route53 un dominio y así es como nos conectamos.\n",
    "\n",
    "Seguridad con Apache Zeppelin en el archivo Shiro.ini se puede hacer configuraciones e integraciones con directorio activo para aumentar la seguridad ya que al momento de iniciar sesión pedirá usuario y clave.\n",
    "\n",
    "https://zeppelin.apache.org/docs/0.8.0/setup/security/shiro_authenticatio\n",
    "\n",
    "### Lecturas recomendadas:\n",
    "\n",
    "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-zeppelin.html\n",
    "\n",
    "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1f8f7",
   "metadata": {},
   "source": [
    "## <a name=\"mark_29\"></a>Demo- Despliegue automático de EMR cluster con cloudformation\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "```yaml\n",
    "AWSTemplateFormatVersion: '2010-09-09'\n",
    "Description: Cloudformation Stack to deploy BigData Enviroment\n",
    "Parameters:\n",
    "    awsRegion:\n",
    "        Description: awsRegion\n",
    "        Default: us-east-1\n",
    "        Type: String\n",
    "    EnvironmentName:\n",
    "        Description: \"A prefix for the resource names.\"\n",
    "        Type: String\n",
    "        Default: OPS-BigData\n",
    "Mappings:\n",
    "    EnvironmentMap:\n",
    "        OPS-BigData-Emr:\n",
    "            EnvironmentName: OPS-BigData\n",
    "            VpcAddress: 10.4.0.0/16\n",
    "            PublicSubNetAAddress: 10.4.0.0/24\n",
    "            PublicSubNetBAddress: 10.4.1.0/24\n",
    "            PrivateSubNetAAddress: 10.4.2.0/24\n",
    "            PrivateSubNetBAddress: 10.4.3.0/24\n",
    "        DEVELOPMENT-BigData-Emr:\n",
    "            EnvironmentName: DEVELOPMENT-BigData\n",
    "            VpcAddress: 10.5.0.0/16\n",
    "            PublicSubNetAAddress: 10.5.0.0/24\n",
    "            PublicSubNetBAddress: 10.5.1.0/24\n",
    "            PrivateSubNetAAddress: 10.5.2.0/24\n",
    "            PrivateSubNetBAddress: 10.5.3.0/24\n",
    "        STAGING-BigData-Emr:\n",
    "            EnvironmentName: STAGING-BigData\n",
    "            VpcAddress: 10.6.0.0/16\n",
    "            PublicSubNetAAddress: 10.6.0.0/24\n",
    "            PublicSubNetBAddress: 10.6.1.0/24\n",
    "            PrivateSubNetAAddress: 10.6.2.0/24\n",
    "            PrivateSubNetBAddress: 10.6.3.0/24\n",
    "        PRODUCTION-BigData-Emr:\n",
    "            EnvironmentName: PRODUCTION-BigData\n",
    "            VpcAddress: 10.7.0.0/16\n",
    "            PublicSubNetAAddress: 10.7.0.0/24\n",
    "            PublicSubNetBAddress: 10.7.1.0/24\n",
    "            PrivateSubNetAAddress: 10.7.2.0/24\n",
    "            PrivateSubNetBAddress: 10.7.3.0/24\n",
    "Resources:\n",
    "    IngestCashoutsStep: \n",
    "        Type: \"AWS::EMR::Step\"\n",
    "        DependsOn: IngestContactsStep\n",
    "        Properties:\n",
    "            ActionOnFailure: \"CONTINUE\"\n",
    "            HadoopJarStep:\n",
    "                Args:\n",
    "                    - \"spark-submit\"\n",
    "                    - \"--py-files\"\n",
    "                    - \"/tmp/file.py\"\n",
    "                Jar: \"command-runner.jar\"\n",
    "            Name: \"IngestCashouts\"\n",
    "            JobFlowId: !Ref EMRClusterV5\n",
    "    EMRClusterV5:\n",
    "        Type: AWS::EMR::Cluster\n",
    "        Properties:\n",
    "            Instances:\n",
    "                MasterInstanceGroup:\n",
    "                    InstanceCount: 1\n",
    "                    InstanceType: \"r4.2xlarge\"\n",
    "                    Market: \"ON_DEMAND\"\n",
    "                    Name: \"prd-Emr-master-BigData\"\n",
    "                CoreInstanceGroup:\n",
    "                    InstanceCount: 2\n",
    "                    InstanceType: \"r4.2xlarge\"\n",
    "                    Market: \"ON_DEMAND\"\n",
    "                    Name: \"prd-Emr-core-BigData-Dev\"\n",
    "                Ec2SubnetId:\n",
    "                    Fn::ImportValue:\n",
    "                        !Join ['-', ['PublicSubNetAId',!FindInMap [ EnvironmentMap, !Ref \"AWS::StackName\", EnvironmentName ]]]\n",
    "                Ec2KeyName: Key-BigData\n",
    "                EmrManagedMasterSecurityGroup:\n",
    "                    Fn::ImportValue:\n",
    "                        !Join ['-', ['PortsForEmrMasterNode',!FindInMap [ EnvironmentMap, !Ref \"AWS::StackName\", EnvironmentName ]]]\n",
    "                EmrManagedSlaveSecurityGroup:\n",
    "                    Fn::ImportValue:\n",
    "                        !Join ['-', ['PortsForEmrCoreNode',!FindInMap [ EnvironmentMap, !Ref \"AWS::StackName\", EnvironmentName ]]]\n",
    "            BootstrapActions:\n",
    "                -\n",
    "                    Name: \"BootstrapEMR\"\n",
    "                    ScriptBootstrapAction:\n",
    "                        Path: \"s3://mybucket/deploy/boot.sh\"\n",
    "            LogUri: \"s3://mybucket/emr/batch/\"\n",
    "            Configurations:\n",
    "            - Classification: hadoop-env\n",
    "              Configurations:\n",
    "              - Classification: export\n",
    "                ConfigurationProperties:\n",
    "                  JAVA_HOME: \"/usr/lib/jvm/java-1.8.0\"\n",
    "            - Classification: spark-env\n",
    "              Configurations:\n",
    "              - Classification: export\n",
    "                ConfigurationProperties:\n",
    "                  JAVA_HOME: \"/usr/lib/jvm/java-1.8.0\"\n",
    "            - Classification: hadoop-log4j\n",
    "              ConfigurationProperties:\n",
    "                    hadoop.log.maxfilesize: 256MB\n",
    "                    hadoop.log.maxbackupindex: '3'\n",
    "                    hadoop.security.log.maxfilesize: 256MB\n",
    "                    hadoop.security.log.maxbackupindex: '3'\n",
    "                    hdfs.audit.log.maxfilesize: 256MB\n",
    "                    hdfs.audit.log.maxbackupindex: '3'\n",
    "                    mapred.audit.log.maxfilesize: 256MB\n",
    "                    mapred.audit.log.maxbackupindex: '3'\n",
    "                    hadoop.mapreduce.jobsummary.log.maxfilesize: 256MB\n",
    "                    hadoop.mapreduce.jobsummary.log.maxbackupindex: '3'\n",
    "            - Classification: hbase-log4j\n",
    "              ConfigurationProperties:\n",
    "                  hbase.log.maxbackupindex: '3'\n",
    "                  hbase.log.maxfilesize: 10MB\n",
    "                  hbase.security.log.maxbackupindex: '3'\n",
    "                  hbase.security.log.maxfilesize: 10MB\n",
    "            - Classification: yarn-site\n",
    "              ConfigurationProperties:\n",
    "                  yarn.log-aggregation.retain-seconds: '43200'\n",
    "            Applications:\n",
    "            - Name: Hadoop\n",
    "            - Name: Hive\n",
    "            - Name: Pig\n",
    "            - Name: Spark\n",
    "            Name: \"Emr-cluster-BCI-BigData\"\n",
    "            JobFlowRole: \"EMR_EC2_DefaultRole\"\n",
    "            ServiceRole: \"EMR_DefaultRole\"\n",
    "            ReleaseLabel: \"emr-5.4.0\"\n",
    "            VisibleToAllUsers: true\n",
    "            Tags:\n",
    "            - Key: Name\n",
    "              Value:\n",
    "                Fn::Join:\n",
    "                - ''\n",
    "                - - emr-instance-\n",
    "                  - Ref: AWS::StackName\n",
    "                  - ''\n",
    "            - Key: Environment\n",
    "            - Key: Stack ID\n",
    "              Value:\n",
    "                Ref: AWS::StackName\n",
    "```\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    " **Explicación paso a paso del YAML para CloudFormation:**\n",
    "\n",
    "**1. Encabezado:**\n",
    "- **AWSTemplateFormatVersion:** Indica la versión del formato de plantilla de CloudFormation (en este caso, 2010-09-09).\n",
    "- **Description:** Breve descripción del propósito de la plantilla.\n",
    "\n",
    "**2. Parámetros:**\n",
    "- **awsRegion:** Define la región de AWS donde se desplegarán los recursos.\n",
    "- **EnvironmentName:** Prefijo para los nombres de los recursos.\n",
    "\n",
    "**3. Mapeos:**\n",
    "- **EnvironmentMap:** Almacena valores específicos para diferentes entornos (OPS-BigData-Emr, DEVELOPMENT-BigData-Emr, etc.).\n",
    "\n",
    "**4. Recursos:**\n",
    "- **IngestCashoutsStep:** --> Este es el nombre del step.\n",
    "    - **Tipo:** AWS::EMR::Step (paso de un clúster EMR).\n",
    "    - **DependsOn:** Indica que depende del paso \"IngestContactsStep\".\n",
    "    - **ActionOnFailure:** Comportamiento en caso de fallo (CONTINUE).\n",
    "    - **HadoopJarStep:** Especificaciones del paso Spark:\n",
    "        - **Args:** Argumentos para la ejecución de Spark.\n",
    "        - **Jar:** Archivo JAR que contiene el código Spark.\n",
    "    - **Name:** Nombre del paso.\n",
    "    - **JobFlowId:** ID del clúster EMR donde se ejecutará el paso.\n",
    "    \n",
    "- **EMRClusterV5:** --> Acá se crea el cluster.\n",
    "    - **Tipo:** AWS::EMR::Cluster (clúster EMR).\n",
    "    - **Properties:** Propiedades del clúster:\n",
    "        - **Instances:** Configuración de las instancias del clúster:\n",
    "            - **MasterInstanceGroup:** Grupo de instancias maestras.\n",
    "            - **CoreInstanceGroup:** Grupo de instancias de núcleo.\n",
    "            - **Ec2SubnetId:** Subred de la VPC donde se desplegarán las instancias.\n",
    "            - **Ec2KeyName:** Par de claves SSH para acceder a las instancias.\n",
    "            - **EmrManagedMasterSecurityGroup:** Grupo de seguridad para las instancias maestras.\n",
    "            - **EmrManagedSlaveSecurityGroup:** Grupo de seguridad para las instancias de núcleo.\n",
    "            Algo a tener en cuentar de estos dos grupos de seguridad, si deplegamos el cluster en un a sub-red pública Cloudformation crea 2 security groups, si desplegamos en una sub-red privada crea un tercer security group para agregar una capa de seguridad.\n",
    "        - **BootstrapActions:** Acciones de arranque del clúster antes de que el cluster quede en status Activo.\n",
    "        - **LogUri:** Ubicación en S3 para almacenar los logs del clúster.\n",
    "        - **Configurations:** Configuraciones adicionales para Hadoop, Spark, log4j y yarn-site.\n",
    "        - **Applications:** Aplicaciones a instalar en el clúster (Hadoop, Hive, Pig, Spark), las versiones dependeran de la versión de EMR utilizada en ReleaseLabel.\n",
    "        - **Name:** Nombre del clúster.\n",
    "        - **JobFlowRole:** Rol de IAM para las instancias del clúster.\n",
    "        - **ServiceRole:** Rol de IAM para el servicio EMR.\n",
    "        - **ReleaseLabel:** Versión de EMR a utilizar.\n",
    "        - **VisibleToAllUsers:** Visibilidad del clúster para todos los usuarios de la cuenta.\n",
    "        - **Tags:** Etiquetas para identificar los recursos del clúster.\n",
    "\n",
    "**En resumen, este YAML define una plantilla de CloudFormation para crear un clúster EMR con dos pasos de Spark: \"IngestContactsStep\" e \"IngestCashoutsStep\".**\n",
    "\n",
    "En el contexto de YAML, `!Join` es una instrucción que se utiliza para unir una lista de valores en una cadena. La instrucción `!Join` tiene dos parámetros obligatorios:\n",
    "\n",
    "* `delimiter`: El carácter o cadena que se utilizará para unir los valores.\n",
    "* `items`: La lista de valores que se unirán.\n",
    "\n",
    "La instrucción `!Join` devuelve una cadena que contiene los valores de la lista, unidos por el carácter o cadena especificado.\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "```yaml\n",
    "fruits:\n",
    "  - apple\n",
    "  - banana\n",
    "  - orange\n",
    "\n",
    "fruit_string: !Join [\" \", fruits]\n",
    "```\n",
    "\n",
    "Este ejemplo crea una lista de frutas llamada `fruits`. Luego, utiliza la instrucción `!Join` para crear una cadena llamada `fruit_string` que contiene los valores de la lista, unidos por un espacio.\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "apple banana orange\n",
    "```\n",
    "\n",
    "La instrucción `!Join` también acepta algunos parámetros opcionales:\n",
    "\n",
    "* `prefix`: Una cadena que se añadirá al principio de la cadena resultante.\n",
    "* `suffix`: Una cadena que se añadirá al final de la cadena resultante.\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "```yaml\n",
    "fruits:\n",
    "  - apple\n",
    "  - banana\n",
    "  - orange\n",
    "\n",
    "fruit_string: !Join [\" \", fruits, \" are delicious!\"]\n",
    "```\n",
    "\n",
    "Este ejemplo utiliza el parámetro `suffix` para añadir la cadena \" are delicious!\" al final de la cadena resultante.\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "apple banana orange are delicious!\n",
    "```\n",
    "\n",
    "La instrucción `!Join` es una herramienta útil para unir listas de valores en una cadena. Se puede utilizar en una variedad de contextos, como la creación de informes, la generación de mensajes personalizados y la manipulación de datos.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de YAML, `!FindInMap` es una función intrínseca utilizada para recuperar un valor de un mapa basado en claves específicas. Este mapa se define previamente en la sección `Mappings` del archivo YAML.\n",
    "\n",
    "**Sintaxis:**\n",
    "\n",
    "```yaml\n",
    "!FindInMap [MapName, Key1, [Key2]]\n",
    "```\n",
    "\n",
    "**Parámetros:**\n",
    "\n",
    "* `MapName`: El nombre del mapa definido en la sección `Mappings`.\n",
    "* `Key1`: La clave principal para buscar el valor deseado.\n",
    "* `Key2`: (Opcional) Una clave secundaria para refinar aún más la búsqueda dentro del valor asociado a la `Key1`. Si no se define, se recuperará el valor principal asociado a la `Key1`.\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "```yaml\n",
    "Mappings:\n",
    "  Regions:\n",
    "    us-east-1:\n",
    "      ami: ami-12345678\n",
    "      instance_type: t2.micro\n",
    "    us-west-2:\n",
    "      ami: ami-98765432\n",
    "      instance_type: t3.medium\n",
    "\n",
    "Resources:\n",
    "  EC2Instance:\n",
    "    Type: AWS::EC2::Instance\n",
    "    Properties:\n",
    "      ImageId: !FindInMap [Regions, !Ref \"AWS::Region\"], ami\n",
    "      InstanceType: !FindInMap [Regions, !Ref \"AWS::Region\"], instance_type\n",
    "```\n",
    "\n",
    "En este ejemplo, se define un mapa llamado `Regions` en la sección `Mappings`. Cada entrada del mapa representa una región de AWS con claves para el AMI ID y el tipo de instancia predeterminado. Luego, en la definición del recurso `EC2Instance`, se utilizan las funciones `!FindInMap` para recuperar el AMI ID y el tipo de instancia específicos basados en la región actual (`!Ref \"AWS::Region\")`.\n",
    "\n",
    "**Puntos importantes:**\n",
    "\n",
    "* `!FindInMap` es sensible a las mayúsculas y minúsculas.\n",
    "* Solo devuelve un valor. Si la combinación de claves no existe en el mapa, se generará un error.\n",
    "* Se puede utilizar dentro de expresiones, funciones y condicionales.\n",
    "\n",
    "**En resumen, !FindInMap es una manera poderosa de reutilizar valores configurados en mapas y mantenerDRY tu código YAML.** Te permite manejar configuraciones regionales o dinámicas de manera eficiente y centralizada.\n",
    "\n",
    "Espero que esta explicación te haya sido útil. Si tienes alguna otra pregunta o necesitas más detalles, no dudes en preguntar.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Como es la dinámica en un ambiente productivo de procesamiento tipo batch (procesamiento de toda una carga historica), este template vive en un repositorio X, entonces para una la task diaria, el repo se conecta a un codepipeline que va a tomar esta task, el codepipeline va desplegar el Cloudformation, el Cloudformation lanza el cluster, el cluster ejecuta todos los steps y al final se elimina. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c97268",
   "metadata": {},
   "source": [
    "## <a name=\"mark_30\"></a>AWS - Lambda\n",
    "\n",
    "## [Indice](#index_01)\n",
    "\n",
    "AWS Lambda es un servicio muy importante y muy usado en el mundo Real-Time, y en este caso lo veremos con especial énfasis en proyectos de BigData.\n",
    "\n",
    "### Cosas a tener en cuenta para AWS Lambda\n",
    "\n",
    "![](img_218.png)\n",
    "\n",
    "### Límite de llamadas concurrente que se van a realizar a la función lambda: \n",
    "\n",
    "Por defecto, por cuenta se puede llegar hasta 1.000 llamadas concurrentes de funciones Lambda. Este valor puede ser expansible a través del soporte hasta 20.000. Por esta razón, es recomendable tener una cuenta para cada ambiente en proyectos de BigData, para que los límites sean completamente independientes.\n",
    "\n",
    "Si manejamos proyectos con una misma cuenta con multiples VPC esa concurrencia se va a dividir en todas las lambdas que tengamos en la cuenta y consumiremos nuestro límite rapidamente. Por eso lo recomendado es trabajar Multi-account, utilizando landing zone en AWS, que tengamos multiples cuentas interconectadas.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de AWS, una zona de aterrizaje (landing zone) es una configuración de infraestructura predefinida que se utiliza para desplegar aplicaciones y servicios en AWS. Las zonas de aterrizaje suelen incluir recursos como VPC, subredes, grupos de seguridad, roles de IAM, instancias EC2, volúmenes EBS y almacenamiento S3.\n",
    "\n",
    "Las zonas de aterrizaje proporcionan una serie de beneficios, entre los que se incluyen:\n",
    "\n",
    "* **Seguridad:** Las zonas de aterrizaje pueden ayudar a mejorar la seguridad de las aplicaciones y servicios al proporcionar una base para la implementación de controles de seguridad.\n",
    "* **Escalabilidad:** Las zonas de aterrizaje pueden ayudar a escalar las aplicaciones y servicios al proporcionar una base para la implementación de patrones de escalabilidad.\n",
    "* **Coste:** Las zonas de aterrizaje pueden ayudar a reducir los costes de las aplicaciones y servicios al proporcionar una base para la implementación de prácticas de gestión de costes.\n",
    "\n",
    "AWS ofrece una serie de herramientas y servicios que pueden ayudar a crear y administrar zonas de aterrizaje, entre los que se incluyen:\n",
    "\n",
    "* **AWS Landing Zone:** Es una plantilla de CloudFormation que se puede utilizar para crear una zona de aterrizaje completa.\n",
    "* **AWS Control Tower:** Es un servicio que proporciona una infraestructura de gestión y seguridad preconfigurada para AWS.\n",
    "* **AWS CloudFormation:** Es un servicio que permite crear y administrar recursos de AWS de forma declarativa.\n",
    "\n",
    "Las zonas de aterrizaje son una herramienta importante para las organizaciones que desean desplegar aplicaciones y servicios en AWS de forma segura, escalable y rentable.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "### Integraciones: \n",
    "\n",
    "AWS Lambda es un servicio que se puede integrar con casi cualquier otro servicio dentro de AWS; en este caso, se puede integrar con Kinesis Firehose. Como sería esa integración, tenemos Kinesis Firehose que hace el delivery de la información, el servicio puede tener una lambda que realice alguna transformación que requiera el endpoint al cual vamos a alimentar, adisionalmente lo podemos utilizar con servicios como Kinesis Datastreams, Elasticsearch, DynamoDB, Redshift, y todo el ecosistema de Big Data.\n",
    "\n",
    "SQS: Cuando se trata de AWS Lambda, suele trabajarse con un flujo muy alto de eventos, por lo cual, es muy recomendable utilizar colas; de manera que si el servicio de Lambda llega a su límite, la cola se va a ir acumulando y Lambda va procesando los eventos sin tener problemas de cuellos de botella. Siendo este el caso, podríamos perder un poco de Real-Time al tener un delay, pero no tendremos delay en la función Lambda, lo cual es muy importante.\n",
    "\n",
    "También podríamos usar SNS, que son notificaciones, pero éstas sí son en tiempo real, lo cual significa que a medida que llega el SNS, la función Lambda lo procesa; si no lo puede procesar, se ejecutan una serie de reintentos, pero es más probable que se generen throttles utilizando SNS que con una cola de SQS.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En el contexto de las funciones de Lambda de AWS, los throttles son mecanismos que limitan la cantidad de solicitudes que una función puede procesar en un periodo de tiempo determinado. Los throttles se utilizan para evitar que las funciones de Lambda consuman demasiados recursos y causen problemas de rendimiento en la plataforma.\n",
    "\n",
    "Existen dos tipos de throttles para las funciones de Lambda:\n",
    "\n",
    "* **Throttles de concurrencia:** Estos throttles limitan la cantidad de solicitudes que una función puede procesar de forma simultánea.\n",
    "* **Throttles de tasa:** Estos throttles limitan la cantidad de solicitudes que una función puede procesar en un periodo de tiempo determinado.\n",
    "\n",
    "Los throttles de concurrencia se aplican a cada instancia de función de Lambda. Por ejemplo, si una función de Lambda tiene un throttle de concurrencia de 10, solo podrá procesar 10 solicitudes de forma simultánea, incluso si hay muchas más solicitudes esperando.\n",
    "\n",
    "Los throttles de tasa se aplican a todas las instancias de función de Lambda de una región de AWS. Por ejemplo, si una función de Lambda tiene un throttle de tasa de 100 solicitudes por segundo, no podrá procesar más de 100 solicitudes por segundo, incluso si hay muchas instancias de función de Lambda disponibles.\n",
    "\n",
    "Los throttles de Lambda se aplican de forma automática y no se pueden desactivar. Sin embargo, se pueden ajustar para satisfacer las necesidades específicas de una función. Por ejemplo, si una función de Lambda requiere procesar un gran número de solicitudes de forma simultánea, se puede aumentar el throttle de concurrencia.\n",
    "\n",
    "Los throttles de Lambda son una parte importante de la arquitectura de la plataforma. Ayudan a garantizar que las funciones de Lambda puedan funcionar correctamente y evitar que causen problemas de rendimiento.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "![](img_219.png)\n",
    "\n",
    "En arquitecturas Real-Time, normalmente se maneja una gran cantidad de funciones Lambda, y orquestar esas funciones y el código dentro de ellas es muy complejo.\n",
    "\n",
    "Deployment: Una de las recomendaciones es optimizar y automatizar el despliegue de código en las funciones Lambda utilizando diferentes servicios de AWS como por ejemplo: \n",
    "\n",
    "    - Ejemplo con CodePipeline: Tenemos todo el código de nuestra lambda en un repositorio, el CodePipeline toma el código lo copia a S3 y desde S3 actualizamos la lambda de una forma completamente controlada, también lo podríamos hacer con Cloudformation depende de la implementación, pero en este ejemplo lo usamos con CodePipeline, Python y la librería de Boto3, al ser una gran cantidad de lambdas aumenta la complejidad de administración de las mismas.\n",
    "    \n",
    "Monitoreo: En sistemas Real-Time tenemos muchas funciones Lambda que procesan mucha información, y muchas veces es muy complejo encontrar errores; para esto, hay diferentes tipos de servicios, uno de ellos es Step Functions, que nos ayuda a orquestar funciones y a identificar demoras en las funciones.\n",
    "\n",
    "También podemos incluir librerías dentro del código de las lambda para monitoreo de ejecución de Código, como por ejemplo Rollbar, una de sus principales funciones es notificar por medio de alertas los errores que ocurren en las funciones lambda enviando la notificación a un dashboard especificando sus causas.\n",
    "\n",
    "AWS cuenta también con servicios como X-Ray, ese servicio lo vamos utilizar cuando vallamos a identificar errores específicos y puntuales dentro de funciones Lambda cuando sospechemos que tenemos una demora. Al activar el servicio comienza a registrar todas las trazas de la lambda con su tiempo de ejecución, de esta forma podemos ver en el servicio cual ejecución se demoró más.\n",
    "\n",
    "Manejo de errores: Cuando procesamos estos flujos de información, puede pasar que tengamos un error en el endpoint y que no alcancemos a enviar toda la información, o que el endpoint esté caído y la cantidad de reintentos no dé a basto. Para esto, podemos configurar colas en las funciones Lambda, de manera que si la función ejecutó ya todos los reintentos y el endpoint no le respondió, puede enviar esa información a una cola llamda \"dead letter queue\" (cola muerta), y de ahí podríamos procesarla con otra función para que la ingrese a un DynamoDB y luego intentar de re-procesar la información hacia su endpoint destino. La idea de este manejo de errores es nunca perder eventos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8242f1b",
   "metadata": {},
   "source": [
    "## <a name=\"mark_31\"></a>Ejemplos AWS- Lambda\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "Explicación de una arquitectura Real Time alimentando un cluster de Elastic Search y un Endpoint de terceros \n",
    "\n",
    "![](img_220.png)\n",
    "\n",
    "Inicia con un flujo de logs que viene de Cloudwatch (**ECS-Environment**), que alimenta una lambda de **DISTRIBUTION o FIND OUT** que tomará todos los flujos que vienen y los distribuirá en multiples **LAMBDAS**, por qué?, porque estas lambdas van a alimentar diferentes endpoints, en este caso los distribuye utilizando **SNS**, sin embargo también podríamos utilizar **SQS**, pero que debemos tener en cuenta, cuando utilicemos **SQS** las lambdas reciben el trigger de **SQS** pero de cola STD (un evento te garatiza que llega al menos una vez pero no que no puede llegar 2 veces, es decir puede que tengamos flujos duplicados). \n",
    "\n",
    "Flujo **ES-1**, se encargan de ingestar en \"ElasticSearch\" todo el indice que necesitamos para visualizarlo en **ES-Kibana 6.0**. (Ejemplo indices de eventos que llegan de producción)\n",
    "\n",
    "Flujo para **lambda IN-VPC**, esta lambda lo distribuye a otro índice completamente diferente de eventos en el mismo cluster de \"Elastic Search\" **ES-2** (Ejemplo indices que llegan con Error)\n",
    "\n",
    "El otro flujo de **lambda IN-VPC**, por qué la lambda está en una VPC? porque para consultar una db Redis lo debo realizar a travéz de un **NAT gateway**, esta lambda tiene que ir a consultar una db Redis (**prd-bigdata-ec-redis**) que contiene en memoria por ejemplo el listado de la cantidad de usuarios y va a evitar que llegen duplicados a nuestro endpoint, entonces **lambda IN-VPC** consulta si el evento pasó o no en la db Redis, si no pasó entonces será ingestado en el endpoint.\n",
    "\n",
    "Siguiente ejemplo, utilizando varias funciones lambda y S3 de forma orquestada, podríamos reemplazar una función completa de EMR, orquestando con lambdas un Modelo de Map Reduce. \n",
    "\n",
    "![](img_221.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17ba7c",
   "metadata": {},
   "source": [
    "## <a name=\"mark_32\"></a>Demo - Creando una lambda para BigData\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "Buscamos en la consola de AWS --> Lambda \n",
    "\n",
    "![](img_222.png)\n",
    "\n",
    "![](img_223.png)\n",
    "\n",
    "![](img_224.png)\n",
    "\n",
    "![](img_225.png)\n",
    "\n",
    "Para proyectos Real Time la recomendación es utilizar SNS o SQS.\n",
    "\n",
    "Layers: cuando manejamos muchas lambdas dentro de una cuenta y muchas librerías que se repiten en las funciones se pueden agregar como capas, es decir se van a administrar una vez y luego la replicaremos en las demás funciones lambda.\n",
    "\n",
    "![](img_226.png)\n",
    "\n",
    "El código de ejecución de la lambda lo dejaremos como está.\n",
    "\n",
    "![](img_227.png)\n",
    "\n",
    "![](img_228.png)\n",
    "\n",
    "Con lo cual la función lambda dentro de su código no tendrá información sencible.\n",
    "\n",
    "![](img_229.png)\n",
    "\n",
    "![](img_230.png)\n",
    "\n",
    "![](img_231.png)\n",
    "\n",
    "![](img_232.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825279c8",
   "metadata": {},
   "source": [
    "## <a name=\"mark_33\"></a>AWS - Athena (carga de info y consulta)\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_233.png)\n",
    "\n",
    "AWS Athena es un servicio de consultas completamente administrado.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Podemos tomar ventajas de realizar consultas SQL a través de Athena sobre la información que esté almacenada en S3. Athena se conectaría a un Glue Catalog, el cual le indicaría dónde está la información almacenada en S3.\n",
    "\n",
    "- Es serverless, es decir, no tendremos cargas de administración o de creación de servidores para utilizar este servicio.\n",
    "\n",
    "- Podemos consultar diferentes tipos de archivos a través de SQL, como por ejemplo, csv, JSON, parquet, ORC, tsv, etc.\n",
    "\n",
    "- Este servicio provee una integración nativa con otros servicios de AWS como Glue, S3, RedShift, DynamoDB y Kinesis.\n",
    "\n",
    "![](img_234.png)\n",
    "\n",
    "- También podemos integrar este servicio haciendo uso de JDBC y ODBC con otras herramientas. Por ejemplo, podríamos trabajar con MySQL Workbrench, y a través de JDBC nos conectaríamos a Athena para realizar consultas a la información almacenada en S3 y poder descargarla.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "JDBC y ODBC son dos interfaces de programación de aplicaciones (API) que permiten a las aplicaciones acceder a bases de datos relacionales. JDBC es una API de Java, mientras que ODBC es una API multiplataforma.\n",
    "\n",
    "**JDBC**\n",
    "\n",
    "JDBC es una API estándar de Java que permite a las aplicaciones Java acceder a bases de datos relacionales. JDBC proporciona una serie de clases y métodos que simplifican la tarea de acceder a datos en una base de datos.\n",
    "\n",
    "Para utilizar JDBC, una aplicación Java debe crear una conexión a la base de datos. Una conexión es un objeto que representa una sesión con la base de datos. Una vez que la aplicación tiene una conexión, puede utilizar los métodos de JDBC para realizar operaciones en la base de datos, como consultar datos, insertar datos, actualizar datos o eliminar datos.\n",
    "\n",
    "JDBC es una API muy flexible que puede utilizarse para acceder a una amplia gama de bases de datos relacionales.\n",
    "\n",
    "**ODBC**\n",
    "\n",
    "ODBC es una API multiplataforma que permite a las aplicaciones acceder a bases de datos relacionales. ODBC proporciona una serie de funciones que simplifican la tarea de acceder a datos en una base de datos.\n",
    "\n",
    "Para utilizar ODBC, una aplicación debe crear un controlador ODBC para la base de datos a la que desea acceder. Un controlador ODBC es un programa que proporciona una interfaz entre la aplicación y la base de datos. Una vez que la aplicación tiene un controlador ODBC, puede utilizar las funciones de ODBC para realizar operaciones en la base de datos.\n",
    "\n",
    "ODBC es una API más compleja que JDBC, pero también es más flexible. ODBC puede utilizarse para acceder a una amplia gama de bases de datos relacionales, incluidas bases de datos que no proporcionan un controlador JDBC.\n",
    "\n",
    "**Comparación de JDBC y ODBC**\n",
    "\n",
    "JDBC y ODBC son dos API que ofrecen funciones similares para acceder a bases de datos relacionales. Sin embargo, existen algunas diferencias importantes entre las dos API:\n",
    "\n",
    "| Característica | JDBC | ODBC |\n",
    "|---|---|---|\n",
    "| Plataforma | Java | Multiplataforma |\n",
    "| Estándar | Sí | No |\n",
    "| Flexibilidad | Menos flexible | Más flexible |\n",
    "| Soporte de bases de datos | Amplio | Amplio |\n",
    "\n",
    "**Conclusión**\n",
    "\n",
    "JDBC y ODBC son dos API que pueden utilizarse para acceder a bases de datos relacionales. JDBC es una API más sencilla que ODBC, pero ODBC es más flexible. La API adecuada para una aplicación concreta depende de las necesidades específicas de la aplicación.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "- Las consultas pueden ser guardadas para utilizarlas más adelante.\n",
    "\n",
    "- En cuanto a seguridad, tenemos permisos granulares a nivel de bases de datos y de tablas.\n",
    "\n",
    "- Es así como este servicio está en una parte muy importante de la cadena de BigData.\n",
    "\n",
    "Ejemplo \n",
    "\n",
    "![](img_235.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed9973",
   "metadata": {},
   "source": [
    "## <a name=\"mark_34\"></a>Demo - Consultando data en S3 con Athena\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "En la consola de AWS --> Athena\n",
    "\n",
    "![](img_236.png)\n",
    "\n",
    "![](img_237.png)\n",
    "\n",
    "![](img_238.png)\n",
    "\n",
    "El costo en es en base a la cantidad de data que se haga en la consulta\n",
    "\n",
    "![](img_239.png)\n",
    "\n",
    "![](img_240.png)\n",
    "\n",
    "Cuando vamos a AWS Glue Data Catalog nos redirige al servicio Glue donde tenemos el Glue Catalog \n",
    "\n",
    "![](img_241.png)\n",
    "\n",
    "_ Athena lo podemos integrar con servicios de visualización como QuickSide.\n",
    "\n",
    "_ Podemos especificar a nivel de role, que se tenga acceso a una db y tablas determinadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4261b00",
   "metadata": {},
   "source": [
    "## <a name=\"mark_35\"></a>AWS - RedShift\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_242.png)\n",
    "\n",
    "Siempre que pensemos en RedShift, tomemos como referencia los términos DataWarehouse y DataLake.\n",
    "\n",
    "Conceptos claves\n",
    "\n",
    "- DataWarehouse: RedShift es básicamente un repositorio de datos completamente centralizado que contiene información de múltiples fuentes dentro de una organización. Este es uno de los servicios más grandes, de los que tiene mayor Billing dentro de AWS.\n",
    "\n",
    "- DataLake: Es un repositorio de almacenamiento que guarda una cantidad muy grande de raw-data, es decir, en formato nativo.\n",
    "\n",
    "- DataMart: Es un subset de DataWarehouse orientado a una tarea específica.\n",
    "\n",
    "![](img_243.png)\n",
    "\n",
    "![](img_244.png)\n",
    "\n",
    "RedShift es una base de datos columnar, es decir, cuando trabajemos con RedShift, la tabla normal se divide en columnas y las consultas son realizadas sobre las columnas. Esto mejora el rendimiento de I/O en los discos, mejora el performance y el tiempo de consulta sobre grandes cantidades de información. Este tipo de estructuras en una base de datos es óptimo para consultas sobre analítica, para transacciones OLAP principalmente.\n",
    "\n",
    "![](img_245.png)\n",
    "\n",
    "Características de RedShift\n",
    "\n",
    "- RedShift es un servicio desplegado dentro de AWS a una escala muy grande (PB o TB der información).\n",
    "\n",
    "- Toda su infraestructura se despliega en un clúster de instancias; el costo dependerá del tamaño de dicho clúster.\n",
    "\n",
    "- Sirve para consultas muy complejas SQL sobre cantidades grandes de datos a nivel columnar. La promesa de valor de RedShift es su increíble tiempo de respuesta en las consultas a gigantescas cantidades de datos (PB o TB der información).\n",
    "\n",
    "![](img_246.png)\n",
    "\n",
    "- Está basado en PostgreSQL.\n",
    "\n",
    "- Está diseñado específicamente para transacciones OLAP; es decir, para transacciones de analítica y procesamiento para proyectos de BI.\n",
    "\n",
    "- Para que RedShift funcione de manera adecuada y cumpla su promesa de valor (tiempos muy cortos sobre una gran cantidad de información), hace compresión de los datos. Esta compresión mejora las actividades de I/O por segundo en los discos.\n",
    "\n",
    "- RedShift utiliza caché para ciertos tipos de consulta y no tener que volver a procesar esa misma información.\n",
    "\n",
    "![](img_247.png)\n",
    "\n",
    "Cual es la diferencia con Athena, en ambos podemos hacer consultas sobre grandes cantidades de datos, y consultas a S3, pero cuando se realizan consultas muy complejas con SQL en Athena puede no ser el mejor servicio, con un mayor tiempo al consultar, también cuando la cantidad de datos comience a crecer exponencialmente pensaremos en Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7e404",
   "metadata": {},
   "source": [
    "## <a name=\"mark_36\"></a>Demo - Creando nuestro primer clúster de RedShift\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "Dentro de la consola de AWS --> Redshift\n",
    "\n",
    "![](img_248.png)\n",
    "\n",
    "![](img_249.png)\n",
    "\n",
    "![](img_250.png)\n",
    "\n",
    "![](img_251.png)\n",
    "\n",
    "![](img_252.png)\n",
    "\n",
    "![](img_253.png)\n",
    "\n",
    "![](img_254.png)\n",
    "\n",
    "![](img_255.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcad7d5",
   "metadata": {},
   "source": [
    "## <a name=\"mark_37\"></a>AWS - Lake Formation\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_256.png)\n",
    "\n",
    "AWS Lake Formation es un servicio que nos ayuda facilitándonos las tareas de transformación (identificación de información, transformación de información, control de acceso y seguridad) a nuestro Data Lake, y permite adicionalmente integración con otros servicios de AWS.\n",
    "\n",
    "La promesa principal de este servicio es reducir las cargas y el tiempo de administración de nuestros Data Lakes dentro de AWS.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Facilita y permite la creación de un Data Lake en cuestión de días con muy buena seguridad; es decir, integra diferentes servicios del ecosistema de AWS para crear un Data Lake de una forma muy fácil y muy rápida.\n",
    "\n",
    "- Tiene integración con diferentes fuentes, hasta On-Premise utilizando JDBC.\n",
    "\n",
    "- Identifica los orígenes y crea las tablas basado en su estructura (ejecutando Crawlers); es decir, este servicio nos ayuda a Crawlear la información con los Crawlers de Glue, identificar la información origen y crear el Glue Catalog.\n",
    "\n",
    "![](img_257.png)\n",
    "\n",
    "- Aparte de identificar la información origen, nos ayuda con los ETLs; es decir, este servicio nos ayuda a orquestar el Crawling, el ETL y si luego necesitamos más Crawling, nos ayudará con ello.\n",
    "\n",
    "- Limpia y elimina información duplicada utilizando una herramienta de Machine Learning llamada FindMatch, la cual, también optimiza y mejora el rendimiento y tiempo de limpieza.\n",
    "\n",
    "- Optimiza las particiones de S3 para consultar más eficientemente la información.\n",
    "\n",
    "![](img_258.png)\n",
    "\n",
    "- Cifrado automático de la información en S3 utilizando SSE-KMS.\n",
    "\n",
    "- En cuanto a accesos, tenemos mayor granularidad que en Athena, al tener control de permisos por usuarios por bases de datos, tablas, columnas y campos.\n",
    "\n",
    "- A nivel de auditoría, tiene integración nativa con CloudTrail, que es un servicio de AWS que registra todas las llamadas a la API.\n",
    "\n",
    "- El cobro de Lake Formation es por cada uno de los servicios integrados a él (Crawling, ETL, Data Catalog, Security Settings & Access Control); Lake Formation como tal no tiene pricing.\n",
    "\n",
    "![](img_259.png)\n",
    "\n",
    "### Funcionalidades\n",
    "\n",
    "_ Owners: Se pueden designar Data Owners para controlar permisos por usuarios.\n",
    "\n",
    "_ Discover: Nos ayuda a descubrir información relevante para implementar análisis utilizando Machine Learning.\n",
    "\n",
    "_ Insights: Podemos utilizar este servicio e integrarlo nativamente con EMR y RedShift para ejecutar analíticas sobre estos datos.\n",
    "\n",
    "![](img_260.png)\n",
    "\n",
    "El pricing de Lake Formation es por cada servicio que sea utilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a834a",
   "metadata": {},
   "source": [
    "## <a name=\"mark_38\"></a>AWS - ElasticSearch\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_261.png)\n",
    "\n",
    "AWS ElasticSearch es un motor de búsqueda basado en Apache Lucene. Busca data estructurada, data tipo JSON y data no estructurada.\n",
    "Para que el servicio funcione se despliega en un clúster en AWS compuesto de varios nodos, en donde se almacena la información, y a través de un nodo maestro, se realizan las consultas a dicha información.\n",
    "Este servicio viene integrado con Logstash y Kibana.\n",
    "\n",
    "![](img_262.png)\n",
    "\n",
    "### Características\n",
    "\n",
    "- **Autenticación**: Hay diferentes formas de integrar la autenticación con este servicio. La más recomendada es utilizar AWS Cognito, que es un servicio que maneja Users Pools. Podemos hacerlo de diferentes formas, crear un grupo de ususarios con usuario y clave completamente diferente a los de IAM para que tengan el servicio a travéz de una autenticación con usuario y password, o los podemos integrar con sus cuentas de correo corporativas.\n",
    "\n",
    "- **Cifrado**: Se puede cifrar la información en reposo y en tránsito con KMS.\n",
    "\n",
    "- **Integración**: Puede recibir información de Kinesis Firehose y de Lambda. Con Kinesis Firehose se puede alimentar de a un índice de ElasticSearch, es decir, si se desea alimentar múltiples índices, se necesitan múltiples Kinesis Firehose.\n",
    "\n",
    "### Conceptos fundamentales\n",
    "\n",
    "![](img_263.png)\n",
    "\n",
    "- **Índice**: Es como una base de datos que almacena información que está llegando a ElasticSearch. Este índice es un nombre lógico que toma esa información y la distribuye en otra estructura llamada onshards.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "En Elasticsearch, un **shard** es una unidad lógica de almacenamiento que contiene datos de un índice. Un índice puede tener uno o más shards, y cada shard se puede replicar en uno o más nodos.\n",
    "\n",
    "Un **onshard** es un término que se utiliza para referirse a un shard que se encuentra en un solo nodo. Esto significa que todos los datos del shard se almacenan en el mismo nodo, y solo ese nodo puede acceder a los datos del shard.\n",
    "\n",
    "La estructura de un shard en Elasticsearch es la siguiente:\n",
    "\n",
    "* **Index:** El índice es un conjunto de datos relacionados.\n",
    "* **Shard:** Un shard es una unidad lógica de almacenamiento que contiene datos de un índice.\n",
    "* **Primary shard:** Un primary shard es el shard principal de un índice. Se encarga de procesar las solicitudes de búsqueda y indexación.\n",
    "* **Replicated shard:** Un replicated shard es una réplica de un primary shard. Se utiliza para proporcionar redundancia y escalabilidad.\n",
    "* **Data:** Los datos de un shard se almacenan en un formato de clave-valor. La clave es un identificador único para el documento, y el valor es el contenido del documento.\n",
    "\n",
    "Los shards se distribuyen entre los nodos de un clúster de Elasticsearch de forma automática. Elasticsearch utiliza un algoritmo de distribución que tiene en cuenta el tamaño de los shards, el número de nodos del clúster y la capacidad de cada nodo.\n",
    "\n",
    "Los shards son una parte importante de la arquitectura de Elasticsearch. Permiten distribuir los datos de forma eficiente y escalable, y proporcionan redundancia para evitar la pérdida de datos.\n",
    "\n",
    "En resumen, la estructura de un shard en Elasticsearch es la siguiente:\n",
    "\n",
    "```\n",
    "Index\n",
    "    |- Primary shard\n",
    "        |- Data\n",
    "    |- Replicated shard\n",
    "        |- Data\n",
    "```\n",
    "\n",
    "Un **onshard** es un shard que se encuentra en un solo nodo. Esto significa que todos los datos del shard se almacenan en el mismo nodo, y solo ese nodo puede acceder a los datos del shard.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "La siguiente descripción muestra las equivalencias para la estructura entre un db relacional y ES:\n",
    "\n",
    "SQL => Databases => Tables => Columns/Rows\n",
    "\n",
    "ES => Indices => Types => Documents with Properties\n",
    "\n",
    "Shards: Siempre que creemos un cluster los debemos tener en cuenta, cuando tenemos un ínidce que es quien va a tener toda la información y a mapearla a ese nombre lógico, ese nombre lógico va a distribuir la información en multiples shards, y los shards van a ser esa entidad que se van a distribuir en todos los nodos del cluster para mantener la información.\n",
    "\n",
    "Cuando dimensionemos un clúster de ElasticSearch, es de vital importancia estimar de forma adecuada la cantidad de shards que utilizaremos. Por qué?, supongamos que creamos un cluster de 4 nodos, si este cluster por la cantidad de información lo configuramos con 5 shards, tendremos un desbalance ya que uno de los nodos quedará con 1 solo shard y si esos shards tienen mucha información, al momento de realizar una redistribución podemos tener problemas de rendimiento.\n",
    "\n",
    "Recomendaciones de AWS\n",
    "\n",
    "Utilizar instancias tipo i, optimizadas para storage.\n",
    "Mantener un shard en promedio de 50 a 150GB.\n",
    "\n",
    "![](img_264.png)\n",
    "\n",
    "![](img_265.png)\n",
    "\n",
    "### Observación: \n",
    "\n",
    "Buscar la documentación para estimar correctamente la cantidad de nodos, la cantidad de storage y la cantidad de shards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4b538",
   "metadata": {},
   "source": [
    "## <a name=\"mark_39\"></a>Demo - Creando nuestro primer clúster de ElasticSearch\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "Dentro de la consola de AWS --> ElasticSearch\n",
    "\n",
    "![](img_266.png)\n",
    "\n",
    "![](img_267.png)\n",
    "\n",
    "![](img_268.png)\n",
    "\n",
    "si seleccinamos instancias tipo \"m\", es posible que más adelante no permita el cifrado con KMS.\n",
    "\n",
    "Para cargas muuuuyyy grandes de datos, AWS recomienda la creación de una instancia Master dedicadas, ya que esta se encarga de tomar la información y distribuirla en los nodos, si desabilitamos esta funcionalidad, por defecto tendremos una instancia maestra. Lo recomendado es comenzar con una instancia maestra y si crecemos realizamos un redimensionamiento, tener en cuenta el tema del pricing.\n",
    "\n",
    "![](img_269.png)\n",
    "\n",
    "Cuando utilizamos instancias del tipo \"i\", el almacenamiento viene determinado, si fueran del tipo \"r\" tendríamos que establecer el storage.\n",
    "\n",
    "![](img_270.png)\n",
    "\n",
    "![](img_271.png)\n",
    "\n",
    "![](img_272.png)\n",
    "\n",
    "![](img_273.png)\n",
    "\n",
    "Para el tercer paso \"Configurar el acceso\" en contraparte a lo que se ve, por buenas prácticas lo recomendado es \"acceso público\" siempre integrarlo con Cognito con un pool de usuarios ya creado para aumentar la seguridad, sin Cognito cualquier persona podría ingresar a nuestro dominio.\n",
    "\n",
    "![](img_274.png)\n",
    "\n",
    "### Proseguimos con el escenario sin Cognito:\n",
    "\n",
    "![](img_275.png)\n",
    "\n",
    "![](img_276.png)\n",
    "\n",
    "![](img_277.png)\n",
    "\n",
    "Revisamos y confirmamos en el último step, comienza a desplegar todos los nodos de nuestro cluster nos brindará 2 endpoint, uno para ElasticSearch y otro para Kibana.\n",
    "\n",
    "La puerta de enlace \"Punto de enlace\" es la que en Kinesis firehose o en nuestra función lambda, configuraremos para hacer la ingesta de información\n",
    "\n",
    "El siguiente ejemplo muestra algunas características de un dominio ya creado \"kibanamach\".\n",
    "\n",
    "![](img_278.png)\n",
    "\n",
    "![](img_279.png)\n",
    "\n",
    "![](img_280.png)\n",
    "\n",
    "![](img_281.png)\n",
    "\n",
    "![](img_282.png)\n",
    "\n",
    "![](img_283.png)\n",
    "\n",
    "Podemos de una forma administrada actualizar el cluster a las versiones más recientes sin tener downtime de datos, al momento de realizar un upgrade domain, es importante tener en cuenta que el dimensionamiento de los shards debe estar completamente adecuado para no tener demoras en la reubicación de la información.\n",
    "\n",
    "![](img_284.png)\n",
    "\n",
    "### Elementos claves de ElasticSearch\n",
    "\n",
    "- El dimensionamiento del Cluster es esencial y fundamental (Cantidad de Shards, almacenamiento y la cantidad de índices)\n",
    "\n",
    "- Completamente integrado con LogStage y Kibana para temas de visualización\n",
    "\n",
    "- Siempre en ambientes productivos se debe habilitar el Cifrado de la data (De nodo a nodo y en reposo)\n",
    "\n",
    "- Una medida extra de seguridad. Hacer uso de Amazo Cognito para que los usuarios que van a trabajar en el cluster les aparezca el usuario y password.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdea97f",
   "metadata": {},
   "source": [
    "## <a name=\"mark_40\"></a>AWS - Kibana, visualización.\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_285.png)\n",
    "\n",
    "AWS Kibana viene integrado en el servicio de AWS ElasticSearch.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Función: Permite visualizar de forma gráfica la información que tenemos en ElasticSearch.\n",
    "\n",
    "- Visualización: Provee diferentes opciones de visualización (mapas de calor, barras, tortas, tendencias, etc.) y permite crear gráficos personalizados a través de consultas específicas que ya tiene el clúster.\n",
    "\n",
    "- Integración: Permite el uso de Plugins de terceros para visualización y analítica. Estos Plugins pueden ser utilizados si despliegas el servicio en una instancia EC2 o en un servidor (al desplegarlo en un servidor ganamos mucha flexibilidad pero perdemos soporte y administración de AWS, en una EC2 aún mantenemos estas características).\n",
    "\n",
    "![](img_286.png)\n",
    "\n",
    "En la consola de AWS --> ElasticSearch, ingresamos con un click al dominio de nuestra preferencia.\n",
    "\n",
    "![](img_287.png)\n",
    "\n",
    "Hacemos click sobre el endpoint Kibana\n",
    "\n",
    "![](img_288.png)\n",
    "\n",
    "![](img_289.png)\n",
    "\n",
    "![](img_290.png)\n",
    "\n",
    "En \"Discover\" vamos a visualizar la data que esté llegando en tiempo real de los indices, recordar alimentar los índices con Kinesis Firehose o con una Lambda que ingeste los objetos directamente al endpoint pero dentro de ElasticSearch.\n",
    "\n",
    "En \"Management\" es donde debemos agregar los índices \n",
    "\n",
    "![](img_291.png)\n",
    "\n",
    "Es donde vamos a checkear que esté llegando nueva data y creamos los índices.\n",
    "\n",
    "![](img_292.png)\n",
    "\n",
    "En \"Visualize\" creamos los dashboard de visualización.\n",
    "\n",
    "En \"Dashboard\" juntamos todos las visualizaciones.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "App manda logs a Cloudwatch o Kinesis Data Streams, esos logs pasan a una función lambda que se reprosesan, lo podemos mandar a un Kinesis Firehose o directamente a cluster de Elastic Search, a penas sean ingestados podemos ver los logs en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912f9a9",
   "metadata": {},
   "source": [
    "## <a name=\"mark_41\"></a>AWS - QuickSight visualización BI.\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_293.png)\n",
    "\n",
    "- AWS QuickSight es el servicio más dedicado a visualización dentro de AWS.\n",
    "\n",
    "- Es un servicio en Cloud de AWS enfocado en Business Intelligence para análisis y visualización de información.\n",
    "\n",
    "- Este servicio cuenta con un cliente para dispositivos móviles.\n",
    "\n",
    "- AWS QuickSight puede escalar hasta 10.000 usuarios, y su cobro es por demanda.\n",
    "\n",
    "### Características\n",
    "\n",
    "![](img_294.png)\n",
    "\n",
    "- Machine Learning: Por detrás, cuenta con un motor de Machine Learning llamado SPICE, que incluye funcionalidades como detección de anomalías, prevención, alertas, sugerencias de visualización, entre otras.\n",
    "\n",
    "- Visualización: Utilizando el API permite realizar el embebido de Dashboards en diferentes sistemas.\n",
    "\n",
    "- Integración: Permite integración con una gran variedad de servicios de datos dentro de AWS y servicios de terceros (hablamos de fuentes/sources).\n",
    "\n",
    "![](img_295.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246de13",
   "metadata": {},
   "source": [
    "## <a name=\"mark_42\"></a>Demo - Visualizando nuestra data con QuickSight.\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "En la consola de AWS --> QuickSight, la primera vez que lo utilicemos veremos.\n",
    "\n",
    "![](img_296.png)\n",
    "\n",
    "![](img_297.png)\n",
    "\n",
    "![](img_298.png)\n",
    "\n",
    "![](img_299.png)\n",
    "\n",
    "Una vez creada la cuenta ...\n",
    "\n",
    "![](img_300.png)\n",
    "\n",
    "![](img_301.png)\n",
    "\n",
    "En este punto elegimos la fuente para alimentar QuickSight\n",
    "\n",
    "![](img_302.png)\n",
    "\n",
    "Una vez creado, podemos compartir el dashboard, el análisis o el dataset.\n",
    "\n",
    "![](img_303.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c5351",
   "metadata": {},
   "source": [
    "## <a name=\"mark_43\"></a>Seguridad en los Datos\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_304.png)\n",
    "\n",
    "- Cifrado: Siempre, y en todos los servicios que lo permitan, debemos habilitar el cifrado. La mayoría de servicios tiene integración con KMS. Podemos crear nuestras propias llaves de KMS y configurarlas en todos los servicios posibles como ElasticSearch, Lambda, Redshift.\n",
    "\n",
    "- Permisos: El permiso más detallado posible, la mayor granularidad que podamos asignar en los permisos va a ser vital. Cuando hablamos de datos, podemos tomar todas las medidas de seguridad y a nivel de permisos es fundamental. Si algún usuario necesitase acceder a la información, debemos grantizar sólo otorgarle acceso a la información específica que requiere con el permiso que requiere; obviamente aumenta un poco la carga de administración, pero todo es en pro de la seguridad.\n",
    "\n",
    "- Servicios: Dentro de AWS, es muy recomendable, para proyectos de BigData, utilizar en mayor medida posible servicios administrados (serverless), que no dependan de servidores; esto reduce la administración, aumenta la alta disponibilidad y el performance de nuestros servicios dentro de AWS.\n",
    "\n",
    "![](img_305.png)\n",
    "\n",
    "- Monitoreo: En todos los servicios debemos garantizar que registren todos los logs de ejecución; el monitoreo es fundamental a nivel de logging y a nivel de performance del servicio. Esto nos puede ayudar para identificar problemas y mejorar el rendimiento de nuestras aplicaciones. El monitoreo de los datos es: ¿dónde están?, ¿quién?, ¿cuándo?, ¿por qué?, etc.\n",
    "\n",
    "- Contingencia: Debemos diseñar todos los servicios con un alto nivel de contingencia; es decir, desplegarlos en múltiples zonas, multi-región, replicar la información entre regiones, pruebas de DRP (Disaster Recovery Plan), almacenar información histórica, etc.\n",
    "\n",
    "- Test: Siempre hagamos pruebas sobre la información que estemos recibiendo, siempre verifiquemos la información antes de irnos a producción, ejecutemos los Pipelines, hagamos pruebas con los servicios, tomemos ventaja de todo lo que nos brinda la nube para utilizar los servicios de BigData y hacer pruebas sobre información que tengamos, pero nunca utilicemos la información de producción para hacer pruebas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dfa47",
   "metadata": {},
   "source": [
    "## <a name=\"mark_44\"></a>AWS Macie, seguridad en los Datos\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "![](img_306.png)\n",
    "\n",
    "AWS Macie es un servicio que se basa en técnicas de aprendizaje automático para conectarse a nuestras fuentes de datos y descubrir, clasificar y proteger datos confidenciales\n",
    "\n",
    "Es un servicio administrado que monitoriza la actividad de acceso a los datos en busca de anomalías y genera alertas.\n",
    "Se encuentra completamente integrado en Amazon S3 para proteger los datos almacenados allí.\n",
    "\n",
    "### Tipos de alertas\n",
    "\n",
    "- Predictivas: Puede detectar cambios de lectura/escritura en un Bucket anómalos al comportamiento que ya ha aprendido antes.\n",
    "\n",
    "- Errores de compliance: Personally Identifiable Information (PII) o credenciales de acceso. Ayuda a tener una visualización completa de la información ingestada sobre si se viola alguna política de protección de información personal; es decir, si se ingresa por ejemplo datos de tarjetas de crédito, credenciales de acceso a cualquier plataforma, etc.\n",
    "\n",
    "- Disruption: Puede identificar cambios bruscos en los servicios que puedan afectar algún otro servicio dentro de BigData. Nos dará una visión completa de los servicios aparte de la visión que nos otorga de los datos.\n",
    "\n",
    "- Ransomware: Detecta si tenemos software potencialmente malintencionado en archivos o de cualquier otra manera.\n",
    "\n",
    "- Suspicious: Detecta accessos a los recursos desde direcciones IP o desde sistemas sospechosos.\n",
    "\n",
    "- Privileges: Identifica intentos de un usuario/rol para obtener privilegios elevados.\n",
    "\n",
    "- Anonymous: Detecta accesos a los recursos tratando de ocultarse tras una identidad verdadera.\n",
    "\n",
    "- Permissions: Identifica recursos sensibles de acuerdo a las políticas permisivas y nos sugiere corregir dichos problemas de permisos.\n",
    "\n",
    "- Data Loss: Detecta anomalías de acceso o riesgos de perder información importante.\n",
    "\n",
    "- Credential: Identifica credenciales de acceso comprometidas; pues si AWS Macie puede leer esas credenciales, claramente es una violación al Data Compliance.\n",
    "\n",
    "- Location: Detecta intentos de acceso a la información desde una ubicación desconocida.\n",
    "\n",
    "- Hosting: Previene el almacenamiento de software riesgoso o malintencionado.\n",
    "\n",
    "Primeramente, inscribimos nuestra cuenta de AWS con Amazon Macie, luego seleccionamos los Buckets en donde se clasifica la información, y por último, empezamos a detectar las alertas. Estas alertas se pueden integrar con múltiples servicios y nos dan una visualización completa de:\n",
    "\n",
    "_ Nuestros datos en AWS, del detalle de nuestros datos como tal, qué archivo?, qué hay dentro del archivo?\n",
    "\n",
    "_ El comportamiento de los archivos, comportamiento de escritura/lectura.\n",
    "\n",
    "_ El comportamiento de los usuarios que interactúan con los datos.\n",
    "\n",
    "![](img_307.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36955ff0",
   "metadata": {},
   "source": [
    "## <a name=\"mark_45\"></a>Demo - Configurando AWS Macie\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "En la consola de AWS --> Macie\n",
    "\n",
    "![](img_308.png)\n",
    "\n",
    "![](img_309.png)\n",
    "\n",
    "![](img_310.png)\n",
    "\n",
    "En \"Integrations\" --> \"Select\"\n",
    "\n",
    "![](img_311.png)\n",
    "\n",
    "![](img_312.png)\n",
    "\n",
    "![](img_313.png)\n",
    "\n",
    "![](img_314.png)\n",
    "\n",
    "Observación: Macie cobra por GB Procesado, luego solo cobra por la data nueva que este llegando.\n",
    "\n",
    "Luego de seleccionar los buckerts --> ADD\n",
    "\n",
    "![](img_315.png)\n",
    "\n",
    "![](img_316.png)\n",
    "\n",
    "![](img_317.png)\n",
    "\n",
    "![](img_318.png)\n",
    "\n",
    "![](img_319.png)\n",
    "\n",
    "![](img_320.png)\n",
    "\n",
    "![](img_321.png)\n",
    "\n",
    "![](img_322.png)\n",
    "\n",
    "![](img_323.png)\n",
    "\n",
    "![](img_324.png)\n",
    "\n",
    "![](img_325.png)\n",
    "\n",
    "Entonces podemos copiar el ARN e integrarlo con CloudWatch Events, para que cuando se despliegue la alerta que hace referencia a ese ARN nos llegue una notificación.\n",
    "\n",
    "Podemos crear alertas nuevas y costumizadas desde \"+ ADD NEW\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39cecb",
   "metadata": {},
   "source": [
    "## <a name=\"mark_46\"></a>Apache Airflow, orquestación y automatización\n",
    "\n",
    "## [Indice](#index_02)\n",
    "\n",
    "Importante: AWS recomienda para orquestar cargas de trabajo \"StepFunctions\", sin embargo este servicio solo puede inicializar un job de Glue aún no tiene desarrollo para ser el orquestador de ETL.\n",
    "\n",
    "![](img_326.png)\n",
    "\n",
    "Con este servicio podremos orquestar y automatizar todo un proyecto de BigData.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Apache Airflow permite crear, monitorear y orquestar los flujos de trabajo o sea la extracción, las multiples transformaciones, la alimentación hacia otros servicios.\n",
    "\n",
    "- Los Pipelines son configurados usando Python.\n",
    "\n",
    "- Es muy flexible, permite modificación de executors, operators y demás entidades dentro de Airflow. Aquí podemos utilizar scripts de bash, scripts de Python, podemos implementar emails para notificaciones, podemos integrarlo con bases de datos relacionales, a brokers como Redis, a colas, etc.\n",
    "\n",
    "- Este servicio es Open Source, por lo cual, se puede implementar en AWS de diferentes formas: trabajarlo con contenedores en AWS o trabajarlo con instancias EC2, pero la mejor opción es trabajarlo dentro de GCP Google Cloud Platform, a través de un servicio llamado Cloud Composer que es una integración de Apache Airflow totalmente administrado. Esta es la mejor opción que existe en el mercado para orquestar y automatizar flujos de proyectos de BigData.\n",
    "\n",
    "![](img_327.png)\n",
    "\n",
    "### Conceptos fundamentales\n",
    "\n",
    "- DAG: Directed Acyclic Grap, es una colección de todas las tareas de las que se requiere que corran con sus dependencias y relaciones. Es así como a través del DAG podemos especificar todo un sistema de dependencias y esas dependencias se pueden ejecutar basados en diferentes sensores.\n",
    "\n",
    "- Operator: Describe una tarea que corre independiente de las otras tareas.\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://airflow.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe77703",
   "metadata": {},
   "source": [
    "## <a name=\"mark_47\"></a>Demo - Creando nuestro primer clúster en Cloud Composer\n",
    "\n",
    "## [Indice](#index_03)\n",
    "\n",
    "Vamos a la consola de GCP --> Cloud Composer.\n",
    "\n",
    "![](img_328.png)\n",
    "\n",
    "![](img_329.png)\n",
    "\n",
    "![](img_330.png)\n",
    "\n",
    "Cantidad de nodos, mínimo nos deja 3.\n",
    "\n",
    "![](img_331.png)\n",
    "\n",
    "![](img_332.png)\n",
    "\n",
    "![](img_333.png)\n",
    "\n",
    "![](img_334.png)\n",
    "\n",
    "![](img_336.png)\n",
    "\n",
    "![](img_335.png)\n",
    "\n",
    "Necesitamos cargar los Dags (dag_platzi.py), hacemos click en el enlace al repositorio de los DAGS, e ingresamos a la carpeta \"dags\".\n",
    "\n",
    "![](img_337.png)\n",
    "\n",
    "![](img_338.png)\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "\n",
    "\n",
    "#Jinja templating is used so that this value are replaced on execution.\n",
    "#time when passed to operators\n",
    "\n",
    "year='{{macros.ds_format(ds, \"%Y-%m-%d\", \"%y\")}}'\n",
    "month='{{macros.ds_format(ds,\"%Y-%m-%d\", \"%m\")}}'\n",
    "day='{{macros.ds_format(ds,\"%Y-%m-%d\", \"%d\")}}'\n",
    "\n",
    "WORKFLOW_DEFAULT_ARGS={\n",
    "    'email':['email@gmail.com','email2@gmail.com'],\n",
    "    'email_on_failure':True,\n",
    "    'email_on_retry':True,\n",
    "    'retries':1,\n",
    "    'retry_delay':timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "\n",
    "dag=DAG(\n",
    "    dag_id='pampa_main_etl',\n",
    "    description='Pampa Main ETL DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    "    start_date=datetime(2017,11,1),\n",
    "    catchup=False,\n",
    "    concurrency=3,\n",
    "    default_args=WORKFLOW_DEFAULT_ARGS\n",
    ")\n",
    "\n",
    "\n",
    "def greeting():\n",
    "    import logging\n",
    "    logging.info('Hello World!')\n",
    "\n",
    "############ Cloudwatch logs to S3 ############\n",
    "cloudwatch = PythonOperator(\n",
    "    task_id='extract_cloudwatch_all',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "############ Mongo snapshots to S3 ############\n",
    "accounts_mongo = PythonOperator(\n",
    "    task_id='extract_mongo_accounts',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "profiles_mongo = PythonOperator(\n",
    "    task_id='extract_mongo_profiles',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "contacts_relateds_mongo = PythonOperator(\n",
    "    task_id='extract_mongo_contacts_relateds',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "user_contacts_mongo = PythonOperator(\n",
    "    task_id='extract_mongo_user_relateds',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "verified_contacts_monmgo = PythonOperator(\n",
    "    task_id='extract_mongo_verified_relateds',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "############ Glue #################\n",
    "payments_glue=PythonOperator(\n",
    "    task_id='glue_process_payments',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "referrals_glue=PythonOperator(\n",
    "    task_id='glue_process_referrals',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "prizes_glue=PythonOperator(\n",
    "    task_id='glue_process_prizes',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cashouts_glue=PythonOperator(\n",
    "    task_id='glue_process_cashouts',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cashins_glue=PythonOperator(\n",
    "    task_id='glue_process_cashins',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "phone_contacts_glue=PythonOperator(\n",
    "    task_id='glue_process_phone_contacts',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "platzi_card_receptions_glue=PythonOperator(\n",
    "    task_id='glue_process_platzi_card_receptions',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "email_verifications_glue=PythonOperator(\n",
    "    task_id='glue_process_email_verifications',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "cloudwatch >> payments_glue\n",
    "cloudwatch >> referrals_glue\n",
    "cloudwatch >> prizes_glue\n",
    "cloudwatch >> cashouts_glue\n",
    "cloudwatch >> cashins_glue\n",
    "cloudwatch >> phone_contacts_glue\n",
    "cloudwatch >> platzi_card_receptions_glue\n",
    "cloudwatch >> email_verifications_glue\n",
    "\n",
    "accounts_glue=PythonOperator(\n",
    "    task_id='glue_process_accounts',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "profile_glue=PythonOperator(\n",
    "    task_id='glue_process_profiles',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "verified_contacts_glue=PythonOperator(\n",
    "    task_id='glue_process_verified_contacts_glue',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "user_contacts_glue=PythonOperator(\n",
    "    task_id='glue_process_user_contacts',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "contacts_relateds_glue=PythonOperator(\n",
    "    task_id='glue_process_contacts_relateds',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "accounts_mongo >> accounts_glue\n",
    "profiles_mongo >> profile_glue\n",
    "verified_contacts_monmgo >> verified_contacts_glue\n",
    "user_contacts_mongo >> user_contacts_glue\n",
    "contacts_relateds_mongo >> contacts_relateds_glue\n",
    "\n",
    "\n",
    "############ aggregates #################\n",
    "\n",
    "user_agg_daily_glue=PythonOperator(\n",
    "    task_id='glue_process_user_agg_daily',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "user_details__glue=PythonOperator(\n",
    "    task_id='glue_process_user_details',\n",
    "    python_callable=greeting,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "**Código \"dag_platzi.py\" paso a paso:**\n",
    "\n",
    "**1. Importaciones:**\n",
    "\n",
    "- **import os:** Importa el módulo `os` para interactuar con el sistema operativo.\n",
    "- **from datetime import datetime, timedelta:** Importa las clases `datetime` y `timedelta` para trabajar con fechas y tiempos.\n",
    "- **from airflow import DAG:** Importa la clase `DAG` de Airflow para definir flujos de trabajo.\n",
    "- **from airflow.operators.python_operator import PythonOperator:** Importa el operador `PythonOperator` para ejecutar código Python dentro de los flujos de trabajo.\n",
    "\n",
    "**2. Variables:**\n",
    "\n",
    "- **year, month, day:** Define variables para almacenar el año, mes y día de la fecha de ejecución del flujo de trabajo utilizando Jinja templating.\n",
    "- **WORKFLOW_DEFAULT_ARGS:** Define argumentos predeterminados para los operadores del flujo de trabajo, como direcciones de correo electrónico para notificaciones, reintentos en caso de fallos y retrasos entre reintentos.\n",
    "\n",
    "**3. Creación del DAG:**\n",
    "\n",
    "- **dag = DAG(** ... **):** Crea un objeto DAG con las siguientes características:\n",
    "    - **dag_id='pampa_main_etl':** Identificador único del flujo de trabajo.\n",
    "    - **description='Pampa Main ETL DAG':** Descripción del flujo de trabajo.\n",
    "    - **schedule_interval=timedelta(days=1):** Indica que se ejecutará diariamente.\n",
    "    - **start_date=datetime(2017,11,1):** Fecha de inicio del flujo de trabajo.\n",
    "    - **catchup=False:** No ejecuta las tareas de fechas anteriores a la fecha de inicio.\n",
    "    - **concurrency=3:** Permite ejecutar hasta 3 tareas simultáneamente.\n",
    "    - **default_args=WORKFLOW_DEFAULT_ARGS:** Establece los argumentos predeterminados definidos anteriormente.\n",
    "\n",
    "**4. Definición de la función 'greeting':**\n",
    "\n",
    "- **def greeting():** Define una función simple que imprime \"Hello World!\" en el log.\n",
    "\n",
    "**5. Tareas de extracción de datos de Cloudwatch a S3:**\n",
    "\n",
    "- **cloudwatch = PythonOperator(** ... **):** Crea una tarea tipo PythonOperator para extraer datos de Cloudwatch a S3.\n",
    "\n",
    "**6. Tareas de extracción de datos de Mongo a S3:**\n",
    "\n",
    "- **accounts_mongo, profiles_mongo, contacts_relateds_mongo, user_contacts_mongo, verified_contacts_monmgo:** Crea tareas tipo PythonOperator para extraer datos de diferentes colecciones de Mongo a S3.\n",
    "\n",
    "**7. Tareas de procesamiento de datos con Glue:**\n",
    "\n",
    "- **payments_glue, referrals_glue, prizes_glue, cashouts_glue, cashins_glue, phone_contacts_glue, platzi_card_receptions_glue, email_verifications_glue, accounts_glue, profile_glue, verified_contacts_glue, user_contacts_glue, contacts_relateds_glue:** Crea tareas tipo PythonOperator para procesar los datos extraídos utilizando Glue.\n",
    "\n",
    "**8. Relaciones entre tareas:**\n",
    "\n",
    "- **cloudwatch >> payments_glue, cloudwatch >> referrals_glue, ...:** Define las dependencias entre las tareas, indicando que la tarea 'cloudwatch' debe ejecutarse antes de las tareas de procesamiento de Glue correspondientes.\n",
    "- **accounts_mongo >> accounts_glue, profiles_mongo >> profile_glue, ...:** Define las dependencias entre las tareas de extracción de Mongo y las tareas de procesamiento de Glue correspondientes.\n",
    "\n",
    "**9. Tareas de agregación de datos:**\n",
    "\n",
    "- **user_agg_daily_glue, user_details__glue:** Crea tareas tipo PythonOperator para realizar agregaciones de datos utilizando Glue.\n",
    "\n",
    "**Resumen:**\n",
    "\n",
    "Este código define un flujo de trabajo en Airflow que extrae datos de Cloudwatch y Mongo, los procesa utilizando Glue y realiza agregaciones de datos. Las tareas se ejecutan de forma secuencial según las dependencias definidas.\n",
    "\n",
    "`*************************************************************************************************************************`\n",
    "\n",
    "Una vez cargado nuestro dag ...\n",
    "\n",
    "![](img_339.png)\n",
    "\n",
    "![](img_340.png)\n",
    "\n",
    "![](img_341.png)\n",
    "\n",
    "![](img_342.png)\n",
    "\n",
    "![](img_343.png)\n",
    "\n",
    "### Importante:\n",
    "\n",
    "Ya AWS tiene un servicio como el de google cloud https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/\n",
    "\n",
    "### Lecturas recomendadas\n",
    "\n",
    "https://cloud.google.com/\n",
    "\n",
    "https://drive.google.com/file/d/1ejcTW6B-ZXSlHpKyikPh-nr7-audhkmw/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2cbde",
   "metadata": {},
   "source": [
    "## <a name=\"mark_48\"></a>Arquitectura de referencia\n",
    "\n",
    "## [Indice](#index_03)\n",
    "\n",
    "![](img_344.png)\n",
    "\n",
    "Todos los días nos llegan a CloudWatch unos logs de una app movil, tenemos un porceso que corre en Python que utiliza boto3 y exporta/extrae esos logs los encripta y los lleva S3 donde se guarda esa data cruda, dos servicios se pueden conectar a S3, Glue o EMR para realizar las transformaciones, una vez realizadas las transformaciones la data vuelve a S3 en su folder destino, con el Glue Catalog desde Athena se pueden generar consultas sobre la data transformada, para orquestar el procesamiento podemos utilizar Apache Airflow.\n",
    "\n",
    "![](img_345.png)\n",
    "\n",
    "Kinesis es alimentado por un cluster de contenedores de SS el cual soporta una app movil, esa app movil manda todos sus logs a Kinesis (como ejemplo para este caso puede recibir 80 millones de registros/dia), luego Lambda Find Out se encarga de recibir todos los registro y a travez de una cola SNS o SQS lo despliega en diferentes servicios, uno de los caminos va hacia un Kinesis Firehose el cual tiene una lambda de transformación, y el otro camino tiene un ElastiCache para evitar los elementos duplicados luego cada camino alimenta diferentes servicios.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8259e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a8bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4dc66fa",
   "metadata": {},
   "source": [
    "john ct\n",
    "\n",
    "favor corregir la respuesta de la pregunta en el examen. una arquitectura que no tiene capa batch?. se respondio kappa , luego kinesis, y luego processing pero de acuerdo al o explicado y leido es KAppa\n",
    "\n",
    "\n",
    "10\n",
    "\n",
    "Responder\n",
    "Milton Garcia\n",
    "Milton Garcia\n",
    "\n",
    "¿Y cuál es la correcta? Yo puse processing y kappa, siendo incorrectas ambas.\n",
    "\n",
    "\n",
    "2\n",
    "JAIME LEONARDO ACOSTA DIAZ\n",
    "JAIME LEONARDO ACOSTA DIAZ\n",
    "\n",
    "En mi opinión la pregunta está mal formulada, dado que luego de varios intentos marqué como respuesta LAMBDA y me la dio como correcta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernel_01",
   "language": "python",
   "name": "data_trans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
